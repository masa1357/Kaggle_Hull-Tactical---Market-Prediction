{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da04843",
   "metadata": {},
   "source": [
    "# Hull Tactical Market Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2358dfe",
   "metadata": {},
   "source": [
    "### Import Libralies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "33e2c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "# Models\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Submission\n",
    "import polars as pl\n",
    "import kaggle_evaluation.default_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7e4c475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "427f4438",
   "metadata": {},
   "outputs": [],
   "source": [
    "INNER_VAL_LEN = 180\n",
    "TRADING_DAYS_PER_YR = 252"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad54b45d",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b41dbf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ RETURNS TO SIGNAL CONFIGS ============\n",
    "MIN_SIGNAL: float = 0.0                         # Minimum value for the daily signal \n",
    "MAX_SIGNAL: float = 2.0                         # Maximum value for the daily signal \n",
    "SIGNAL_MULTIPLIER: float = 7.5                 # Multiplier of the OLS market forward excess returns predictions to signal \n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    signal_multiplier: float \n",
    "    min_signal : float = MIN_SIGNAL\n",
    "    max_signal : float = MAX_SIGNAL\n",
    "\n",
    "ret_signal_params = RetToSignalParameters(\n",
    "    signal_multiplier= SIGNAL_MULTIPLIER\n",
    ")\n",
    "\n",
    "def convert_ret_to_signal(\n",
    "    ret_arr: np.ndarray,\n",
    "    params: RetToSignalParameters,\n",
    "    signal_multiplier=None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts raw model predictions (expected returns) into a trading signal.\n",
    "\n",
    "    Args:\n",
    "        ret_arr (np.ndarray): The array of predicted returns.\n",
    "        params (RetToSignalParameters): Parameters for scaling and clipping the signal.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The resulting trading signal, clipped between min and max values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 予測値を基準に，投資戦略シグナルに変換\n",
    "    # ret * signal_multiplier + 1 を min_signal ~ max_signal の範囲にクリップ\n",
    "    if signal_multiplier is None:\n",
    "        multi = params.signal_multiplier    \n",
    "    else:\n",
    "        multi = signal_multiplier\n",
    "    \n",
    "    ret = np.clip(\n",
    "        ret_arr * multi + 1,\n",
    "        params.min_signal, \n",
    "        params.max_signal\n",
    "    )\n",
    "\n",
    "    if ret.size < 20:\n",
    "        print(f\"Strategy:\")\n",
    "        for i, value in enumerate(ret): print(f'  {i}: {value:.4f}')\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "a444b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy:\n",
      "  0: 2.0000\n",
      "  1: 1.7500\n",
      "  2: 2.0000\n",
      "  3: 0.0000\n",
      "  4: 2.0000\n"
     ]
    }
   ],
   "source": [
    "# convert_ret_to_signalの動作確認\n",
    "# 20個の乱数(0~1)\n",
    "hoge = convert_ret_to_signal(np.array([5, 0.1, 0.3, -0.2, 1.3]), ret_signal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "dfc2f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ LOAD DATA ============\n",
    "# プラットフォームがkaggleかローカルかで分岐\n",
    "if os.getenv('KAGGLE_KERNEL_RUN_TYPE') is not None:\n",
    "    # Kaggle上\n",
    "    DATA_PATH: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    DATA_PATH: Path = BASE_PATH / 'data'\n",
    "\n",
    "\n",
    "train = pd.read_csv(DATA_PATH / \"train.csv\")\n",
    "test = pd.read_csv(DATA_PATH / \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ef6c12",
   "metadata": {},
   "source": [
    "### Scoreing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "862dff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    # Custom error to show messages to participants\n",
    "    pass\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, intermediate_res:list = []) -> float:\n",
    "    \"\"\"\n",
    "    Calculates a custom evaluation metric (volatility-adjusted Sharpe ratio).\n",
    "\n",
    "    This metric penalizes strategies that take on significantly more volatility\n",
    "    than the underlying market.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated adjusted Sharpe ratio.\n",
    "    \"\"\"\n",
    "    solution = solution.copy().reset_index(drop=True)\n",
    "    submission = submission.copy().reset_index(drop=True)\n",
    "    solution['position'] = submission['prediction']\n",
    "\n",
    "    # ありえない値を除外する (0 <= position <= 2)\n",
    "        # 0 means that we don't invest in S & P at all but get only the risk-free rate.\n",
    "        # 1 means that we invest all our money in S & P.\n",
    "        # 2 means that we invest twice our capital in S & P while taking a credit at the risk-free rate.\n",
    "        # -> つまり，普通に預金するか，S&Pに投資するか，S&Pに2倍レバレッジで投資するか（借金）の割合\n",
    "    if solution['position'].max() > MAX_SIGNAL:\n",
    "        raise ParticipantVisibleError(f'Position of {solution[\"position\"].max()} exceeds maximum of {MAX_SIGNAL}')\n",
    "    if solution['position'].min() < MIN_SIGNAL:\n",
    "        raise ParticipantVisibleError(f'Position of {solution[\"position\"].min()} below minimum of {MIN_SIGNAL}')\n",
    "\n",
    "    # Calculate strategy returns\n",
    "    # フェデラルファンド金利(利息) * (1-予測値) + 予測値 * S&P500の翌日のリターン = 戦略のリターン(割合)\n",
    "    solution['strategy_returns'] = solution['risk_free_rate'] * (1 - solution['position']) + solution['position'] * solution['forward_returns']\n",
    "\n",
    "    # Calculate strategy's Sharpe ratio\n",
    "    # リターンとその標準偏差を用いてシャープレシオ（リスクあたりの効率）を計算\n",
    "    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate'] # 超過リターン -> 今回の戦略で得た割合から，リスクフリー時の割合を引いた分\n",
    "    strategy_excess_cumulative = (1 + strategy_excess_returns).prod() # 累積超過リターン -> 全期間の超過リターンをかけ合わせた分(1+で倍率に変換)\n",
    "    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solution)) - 1 # 平均超過リターン -> 複利は幾何平均で求める． また，倍率から割合に戻してる\n",
    "    strategy_std = solution['strategy_returns'].std() # リターンの標準偏差\n",
    "\n",
    "    trading_days_per_yr = 252 # 1年あたりの取引日数(固定値)\n",
    "    if strategy_std == 0:\n",
    "        raise ZeroDivisionError\n",
    "    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr) # 年率換算したシャープレシオ. sqrt(252)をかけることで年率換算している（統計的な性質らしい）\n",
    "    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)  # 年率換算したボラティリティ(価格変動率)\n",
    "\n",
    "    # Calculate market return and volatility\n",
    "    # S&P500に投資し続けた場合のリターンとボラティリティを計算\n",
    "    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate'] # S&P500が利息を上回る割合\n",
    "    market_excess_cumulative = (1 + market_excess_returns).prod() # ↑の累積\n",
    "    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solution)) - 1 # train: 0.0003066067595838273 幾何平均，割合化\n",
    "    market_std = solution['forward_returns'].std() # S&P500のリターンの標準偏差\n",
    "    \n",
    "    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100) # train: 16.748459963166347 %\n",
    "    \n",
    "    # Calculate the volatility penalty\n",
    "    # ボラティリティペナルティを計算\n",
    "    # -> 市場のボラティリティの1.2倍を超える場合のペナルティ\n",
    "    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "    vol_penalty = 1 + excess_vol\n",
    "\n",
    "    # Calculate the return penalty\n",
    "    # リターンペナルティを計算\n",
    "    # -> 市場のリターンを下回る場合のペナルティ\n",
    "    return_gap = max(\n",
    "        0,\n",
    "        (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr,\n",
    "    )\n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "\n",
    "    # Adjust the Sharpe ratio by the volatility and return penalty\n",
    "    # ペナルティ値の反映\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "\n",
    "    # print(\"strategy_excess_returns NaN数:\", solution['strategy_returns'].isna().sum())\n",
    "    # print(\"strategy_std:\", strategy_std)\n",
    "    # print(\"strategy_excess_cumulative:\", strategy_excess_cumulative)\n",
    "    # print(\"market_excess_cumulative:\", market_excess_cumulative)\n",
    "    # print(\"adjusted_sharpe:\", adjusted_sharpe)\n",
    "    try:\n",
    "        intermediate_res.append((strategy_mean_excess_return, strategy_std, sharpe, vol_penalty, return_penalty)) # 各値を記録(debug)\n",
    "        return min(float(adjusted_sharpe), 1_000_000), intermediate_res # float変換，上限100万\n",
    "    except NameError:\n",
    "        return min(float(adjusted_sharpe), 1_000_000) # float変換，上限100万"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836dbfa5",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e17aa132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== ユーティリティ ======\n",
    "def _annualize(sigma_daily: np.ndarray) -> np.ndarray:\n",
    "    return sigma_daily * np.sqrt(252.0)\n",
    "\n",
    "def _rolling_vol_no_leak(fr: pd.Series, window_size: int) -> np.ndarray:\n",
    "    # center=False で未来不参照。序盤は expanding で埋める（過去のみ）\n",
    "    roll = fr.rolling(window=window_size, min_periods=window_size, center=False).std()\n",
    "    expd = fr.expanding(min_periods=2).std()\n",
    "    vol = roll.combine_first(expd).bfill(limit=0)\n",
    "    return vol.to_numpy()\n",
    "\n",
    "def _ewma_vol_series(fr: pd.Series, lam: float = 0.94, min_periods: int = 2) -> np.ndarray:\n",
    "    # adjust=False で逐次（未来を見ない）\n",
    "    var = (fr**2).ewm(alpha=1 - lam, adjust=False, min_periods=min_periods).mean()\n",
    "    return np.sqrt(var).to_numpy()\n",
    "\n",
    "def _make_sigma_for_period(fr_all: np.ndarray, start: int, end: int,\n",
    "                           mode=\"ewma\", lam=0.94, window=30) -> np.ndarray:\n",
    "    \"\"\"[start, end) 用 σ_t を直前の過去を使って作る（リークなし）。\"\"\"\n",
    "    warm = max(window, 20)\n",
    "    prefix_start = max(0, start - warm)\n",
    "    fr_prefix = fr_all[prefix_start:start]\n",
    "    fr_period  = fr_all[start:end]\n",
    "    fr_concat  = np.concatenate([fr_prefix, fr_period])\n",
    "    s = pd.Series(fr_concat)\n",
    "    if mode == \"rolling\":\n",
    "        sigma_all = _rolling_vol_no_leak(s, window_size=window)\n",
    "    elif mode == \"ewma\":\n",
    "        sigma_all = _ewma_vol_series(s, lam=lam, min_periods=max(2, window//4))\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'rolling' or 'ewma'\")\n",
    "    return sigma_all[-(end - start):]\n",
    "\n",
    "# ====== Phase1: 静的校正（z化）＋ soft-clip ======\n",
    "def compute_z_calibration(y_pred_inner: np.ndarray, std_scale: float = 2.0) -> tuple[float, float]:\n",
    "    \"\"\"z 正規化に使う (b, T)。b: 中央値, T: std_scale×標準偏差（ゼロ除算ガード）\"\"\"\n",
    "    y = np.asarray(y_pred_inner).reshape(-1)\n",
    "    b = float(np.median(y))\n",
    "    s = float(np.std(y, ddof=1))\n",
    "    T = max(s * std_scale, 1e-8)\n",
    "    return b, T\n",
    "\n",
    "def choose_m_soft_by_clip(z_inner: np.ndarray,\n",
    "                          T_soft: float = 1.0,\n",
    "                          target_clip: float = 0.22,\n",
    "                          m_bounds: tuple[float, float] = (0.5, 5.0)) -> float:\n",
    "    \"\"\"\n",
    "    inner の z 分布から m を自動選定。\n",
    "    1 + m*tanh(z/T_soft) の 0/2 クリップ率が target_clip に近い m を選ぶ。\n",
    "    \"\"\"\n",
    "    z = np.asarray(z_inner).reshape(-1)\n",
    "\n",
    "    def clip_rate_for(m: float) -> float:\n",
    "        # 端貼り付き条件: m * tanh(|z|/T_soft) >= 1\n",
    "        if m <= 1.0:\n",
    "            return 0.0\n",
    "        thr = T_soft * np.arctanh(1.0 / m)\n",
    "        return float(np.mean(np.abs(z) >= thr))\n",
    "\n",
    "    lo, hi = m_bounds\n",
    "    grid = np.linspace(lo, hi, 20)\n",
    "    vals = np.array([clip_rate_for(m) for m in grid])\n",
    "    m0 = float(grid[np.argmin(np.abs(vals - target_clip))])\n",
    "\n",
    "    m_lo, m_hi = max(lo, m0 - 0.5), min(hi, m0 + 0.5)\n",
    "    for _ in range(12):  # 小さな二分探索\n",
    "        m_mid = 0.5 * (m_lo + m_hi)\n",
    "        cr = clip_rate_for(m_mid)\n",
    "        if cr > target_clip:\n",
    "            m_lo = m_mid\n",
    "        else:\n",
    "            m_hi = m_mid\n",
    "    return float(np.clip(0.5 * (m_lo + m_hi), *m_bounds))\n",
    "\n",
    "def soft_clip_from_z(z: np.ndarray, m: float, T_soft: float,\n",
    "                     min_signal: float, max_signal: float) -> np.ndarray:\n",
    "    \"\"\"p = clip( 1 + m * tanh(z / T_soft), [min_signal, max_signal] )\"\"\"\n",
    "    raw = 1.0 + m * np.tanh(z / T_soft)\n",
    "    return np.clip(raw, min_signal, max_signal)\n",
    "\n",
    "# ====== Phase2: 信頼度連動レバ（|z|） ======\n",
    "def conf_m_from_z_abs(z: np.ndarray, m_lo: float = 1.0, m_hi: float = 3.0, Tm: float = 1.2) -> np.ndarray:\n",
    "    \"\"\"|z| が小さいとき m→m_lo, 大きいと m→m_hi\"\"\"\n",
    "    return m_lo + (m_hi - m_lo) * np.tanh(np.abs(z) / Tm)\n",
    "\n",
    "# ====== Phase4: ボラ・ターゲティング ======\n",
    "def vol_target_scaler(sigma_daily: np.ndarray, gamma: float = 1.10, lev_cap: float = 2.0) -> np.ndarray:\n",
    "    \"\"\"ℓ_t = min(lev_cap, σ* / (σ_t^ann + eps)), σ* = gamma * median(σ^ann)\"\"\"\n",
    "    sigma_ann = _annualize(sigma_daily)\n",
    "    sigma_ref = np.median(sigma_ann)\n",
    "    sigma_star = gamma * sigma_ref\n",
    "    return np.minimum(lev_cap, sigma_star / (sigma_ann + 1e-6))\n",
    "\n",
    "# ====== 推論パイプ（z → soft-clip → confidence → vol target） ======\n",
    "def make_allocation_from_predictions(\n",
    "    y_pred: np.ndarray,\n",
    "    *,\n",
    "    b_z: float,\n",
    "    T_z: float,\n",
    "    T_soft: float,\n",
    "    m_soft: float,\n",
    "    conf_params: tuple[float, float, float],  # (m_lo, m_hi, Tm)\n",
    "    sigma_daily: np.ndarray | None,\n",
    "    gamma: float,\n",
    "    lev_cap: float,\n",
    "    min_signal: float,\n",
    "    max_signal: float\n",
    ") -> np.ndarray:\n",
    "    \"\"\"1本のパイプで allocation を作る（リークしない前提の σ を渡すこと）。\"\"\"\n",
    "    # z 化\n",
    "    z = (y_pred - b_z) / T_z\n",
    "    # 基本 soft-clip（中心1.0）\n",
    "    p_soft = soft_clip_from_z(z, m=m_soft, T_soft=T_soft,\n",
    "                              min_signal=min_signal, max_signal=max_signal)\n",
    "    # 信頼度連動レバ（弱めから開始）\n",
    "    m_lo, m_hi, Tm = conf_params\n",
    "    m_conf = conf_m_from_z_abs(z, m_lo=m_lo, m_hi=m_hi, Tm=Tm)\n",
    "    p_conf = np.clip(1.0 + (m_conf * np.tanh(z / T_soft)), min_signal, max_signal)\n",
    "\n",
    "    # 2段構えにしたい場合は p_soft と p_conf を混合しても良いが、まずは p_conf を採用\n",
    "    p = p_conf\n",
    "\n",
    "    # ボラ・ターゲティング（σが渡されない場合はスキップ）\n",
    "    if sigma_daily is not None:\n",
    "        ell_t = vol_target_scaler(sigma_daily, gamma=gamma, lev_cap=lev_cap)\n",
    "        p = np.clip((p - 1.0) * ell_t + 1.0, min_signal, max_signal)\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "4d1585ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== クロスバリデーション（時系列CV + 最後180の診断） ======\n",
    "score_list_dict = {}\n",
    "\n",
    "def cross_validate(\n",
    "    allocation_model,\n",
    "    label: str = \"\",\n",
    "    min_train_size: int = 3000,\n",
    "    test_size: int = 180,\n",
    "    *,\n",
    "    inner_len: int = 720,          # INNER_VAL_LEN 相当\n",
    "    vol_mode: str = \"ewma\",\n",
    "    lambda_ewma: float = 0.94,\n",
    "    window_size: int = 30,\n",
    "    std_scale: float = 2.0,\n",
    "    T_soft_init: float = 1.0,\n",
    "    target_clip: float = 0.22,\n",
    "    conf_params: tuple = (1.0, 3.0, 1.2),  # (m_lo, m_hi, Tm)\n",
    "    gamma: float = 1.10,\n",
    "    lev_cap: float = 2.0\n",
    "):\n",
    "    \"\"\"\n",
    "    時系列を考慮したCV。\n",
    "    - inner から (b,T) と m_soft を決定（リークなし）\n",
    "    - test & 最後180 を同一パイプで評価\n",
    "    \"\"\"\n",
    "    n = len(train)\n",
    "    oof = np.full(n, np.nan, dtype=float)\n",
    "    score_list, val_list, intermediate_res = [], [], []\n",
    "\n",
    "    # 特徴列\n",
    "    drop_cols = [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"market_forward_excess_returns\"]\n",
    "    feature_cols = [c for c in train.columns if c not in drop_cols]\n",
    "    X_all = train[feature_cols]\n",
    "    y_all = train[\"forward_returns\"].to_numpy()\n",
    "    rfr_all = train[\"risk_free_rate\"].to_numpy()\n",
    "\n",
    "    # 最後180（LB）\n",
    "    val_start = max(0, n - 180)\n",
    "    X_val = X_all.iloc[val_start:]\n",
    "    v_sol = pd.DataFrame({\n",
    "        \"forward_returns\": y_all[val_start:],\n",
    "        \"risk_free_rate\":  rfr_all[val_start:],\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    for fold, test_start in enumerate(range(n - test_size, min_train_size, -test_size)):\n",
    "        print(Fore.CYAN + f\"=== Fold {fold} Test start at {test_start} ===\" + Style.RESET_ALL)\n",
    "        test_end = test_start + test_size\n",
    "\n",
    "        # ---- split ----\n",
    "        X_train = X_all.iloc[:test_start]\n",
    "        X_test  = X_all.iloc[test_start:test_end]\n",
    "        y_test  = y_all[test_start:test_end]   # debug用途\n",
    "        sol_test = pd.DataFrame({\n",
    "            \"forward_returns\": y_all[test_start:test_end],\n",
    "            \"risk_free_rate\":  rfr_all[test_start:test_end],\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "        inner_start = max(0, test_start - inner_len)\n",
    "        X_inner = X_all.iloc[inner_start:test_start]\n",
    "        y_inner = y_all[inner_start:test_start]\n",
    "        sol_inner = pd.DataFrame({\n",
    "            \"forward_returns\": y_all[inner_start:test_start],\n",
    "            \"risk_free_rate\":  rfr_all[inner_start:test_start],\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "        # ---- fit ----\n",
    "        allocation_model.fit(X_train, y_all[:test_start])\n",
    "\n",
    "        # ---- inner: z校正 & m_soft 決定 ----\n",
    "        y_pred_inner = allocation_model.predict(X_inner)\n",
    "        b_z, T_z = compute_z_calibration(y_pred_inner, std_scale=std_scale)\n",
    "        z_inner = (y_pred_inner - b_z) / T_z\n",
    "        T_SOFT = T_soft_init\n",
    "        M_SOFT = choose_m_soft_by_clip(z_inner, T_soft=T_SOFT, target_clip=target_clip)\n",
    "\n",
    "        # ---- test: 予測 → 配分 ----\n",
    "        y_pred_test = allocation_model.predict(X_test)\n",
    "        sigma_test_daily = _make_sigma_for_period(y_all, test_start, test_end,\n",
    "                                                  mode=vol_mode, lam=lambda_ewma, window=window_size)\n",
    "        allocation_list = make_allocation_from_predictions(\n",
    "            y_pred_test,\n",
    "            b_z=b_z, T_z=T_z,\n",
    "            T_soft=T_SOFT, m_soft=M_SOFT,\n",
    "            conf_params=conf_params,\n",
    "            sigma_daily=sigma_test_daily,\n",
    "            gamma=gamma, lev_cap=lev_cap,\n",
    "            min_signal=ret_signal_params.min_signal,\n",
    "            max_signal=ret_signal_params.max_signal,\n",
    "        )\n",
    "\n",
    "        # ---- 評価（test fold）----\n",
    "        sub_test = pd.DataFrame({\"prediction\": allocation_list}).reset_index(drop=True)\n",
    "        val_score, intermediate_res = score(sol_test, sub_test, \"\", intermediate_res)\n",
    "\n",
    "        # ---- 最後180（LB診断）----\n",
    "        y_pred_val = allocation_model.predict(X_val)\n",
    "        sigma_val_daily = _make_sigma_for_period(y_all, val_start, n,\n",
    "                                                 mode=vol_mode, lam=lambda_ewma, window=window_size)\n",
    "        val_allocation = make_allocation_from_predictions(\n",
    "            y_pred_val,\n",
    "            b_z=b_z, T_z=T_z,\n",
    "            T_soft=T_SOFT, m_soft=M_SOFT,\n",
    "            conf_params=conf_params,\n",
    "            sigma_daily=sigma_val_daily,\n",
    "            gamma=gamma, lev_cap=lev_cap,\n",
    "            min_signal=ret_signal_params.min_signal,\n",
    "            max_signal=ret_signal_params.max_signal,\n",
    "        )\n",
    "        sub_val = pd.DataFrame({\"prediction\": val_allocation}).reset_index(drop=True)\n",
    "        lb_score, inter2 = score(v_sol, sub_val, \"\", intermediate_res)\n",
    "\n",
    "        # ---- ログ ----\n",
    "        clip0 = np.mean(allocation_list <= ret_signal_params.min_signal) * 100\n",
    "        clip2 = np.mean(allocation_list >= ret_signal_params.max_signal) * 100\n",
    "        if inter2:\n",
    "            strat_mu, strat_std, sharpe, vol_pen, ret_pen = inter2[-1]\n",
    "            print(f\"[last180] sharpe={sharpe:.3f} vol_pen={vol_pen:.2f} ret_pen={ret_pen:.2f}\")\n",
    "            lo = np.mean(val_allocation <= ret_signal_params.min_signal)\n",
    "            hi = np.mean(val_allocation >= ret_signal_params.max_signal)\n",
    "            print(f\"[last180] clip@0={lo:.2%}, clip@2={hi:.2%}\")\n",
    "        print(f\"[inner] M_SOFT={M_SOFT:.3f}, T_SOFT={T_SOFT:.2f}, target clip≈{int(target_clip*100)}%\")\n",
    "        print(f\"[test ] clip@0={clip0:.2f}% clip@2={clip2:.2f}%\")\n",
    "\n",
    "        display(HTML(\n",
    "            f\"<p style='color: orange'>\"\n",
    "            f\"train(:{test_start:4}) test({test_start:4}:{test_end:4})<br>\"\n",
    "            f\"val_score: {val_score:6.3f}<br>\"\n",
    "            f\"score(submission): {lb_score:.6f}<br>\"\n",
    "            f\"z-calib: b={b_z:.3e}, T={T_z:.3e}, M_SOFT={M_SOFT:.3f}, T_SOFT={T_SOFT:.2f}\"\n",
    "            f\"</p>\"\n",
    "        ))\n",
    "\n",
    "        oof[test_start:test_end] = allocation_list\n",
    "        score_list.append(val_score)\n",
    "        val_list.append(lb_score)\n",
    "\n",
    "    # ===== 集計表示 =====\n",
    "    submit_model = allocation_model\n",
    "    display(HTML('<h2 style=\"text-align:center;color:orange\">======== Result ========</h2>'))\n",
    "    avg_score = float(np.nanmean(score_list)) if len(score_list) else np.nan\n",
    "    print(f\"{label} Average Validation Score: {avg_score:.6f}\")\n",
    "\n",
    "    mask = np.isfinite(oof)\n",
    "    if np.any(mask):\n",
    "        solution_all = pd.DataFrame({\n",
    "            \"forward_returns\": y_all[mask],\n",
    "            \"risk_free_rate\":  rfr_all[mask],\n",
    "        }).reset_index(drop=True)\n",
    "        submission_all = pd.DataFrame({'prediction': oof[mask]}).reset_index(drop=True)\n",
    "        overall_score, inter_all = score(solution_all, submission_all, '', [])\n",
    "        vol_penalty = inter_all[-1][3] if inter_all else np.nan\n",
    "        return_penalty = inter_all[-1][4] if inter_all else np.nan\n",
    "        print(f\"{label} Overall Validation Score: {overall_score:.6f} vol_penalty={vol_penalty:.2f} return_penalty={return_penalty:.2f}\")\n",
    "    else:\n",
    "        print(f\"{label} Overall Validation Score: NaN (no valid OOF)\")\n",
    "\n",
    "    score_list_dict[label] = score_list\n",
    "    if score_list:\n",
    "        print(f\"{label} First(Test) Fold Validation Score: {score_list[0]:.6f}\")\n",
    "    if val_list:\n",
    "        print(Fore.YELLOW + f\"All(Test) Fold Validation Score : {(sum(val_list)/len(val_list)):6.3f}\" + Style.RESET_ALL)\n",
    "\n",
    "    # ヒスト\n",
    "    if np.any(mask):\n",
    "        vals = oof[mask]\n",
    "        vmin, vmax = float(np.min(vals)), float(np.max(vals))\n",
    "        if vmin == vmax: vmax = vmin + 1e-6\n",
    "        bins = np.linspace(vmin, vmax, 50)\n",
    "        plt.figure(figsize=(6, 2))\n",
    "        plt.hist(vals, bins=bins, density=False, color='c', edgecolor='k', linewidth=0.5)\n",
    "        plt.title(f'Allocation histogram of {label}')\n",
    "        plt.gca().get_yaxis().set_visible(False)\n",
    "        plt.xlim(vmin, vmax)\n",
    "        plt.show()\n",
    "        print(f\"Range of predictions: [{vmin:.6f}, {vmax:.6f}]\")\n",
    "\n",
    "    return submit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "a3ca9afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 0 Test start at 8810 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002600 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21575\n",
      "[LightGBM] [Info] Number of data points in the train set: 8810, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000468\n",
      "[last180] sharpe=0.375 vol_pen=1.04 ret_pen=1.00\n",
      "[last180] clip@0=0.00%, clip@2=0.00%\n",
      "[inner] M_SOFT=1.421, T_SOFT=1.00, target clip≈22%\n",
      "[test ] clip@0=0.00% clip@2=0.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style='color: orange'>train(:8810) test(8810:8990)<br>val_score:  0.362<br>score(submission): 0.362382<br>z-calib: b=5.080e-04, T=1.742e-02, M_SOFT=1.421, T_SOFT=1.00</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 1 Test start at 8630 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002162 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21573\n",
      "[LightGBM] [Info] Number of data points in the train set: 8630, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000460\n",
      "[last180] sharpe=0.149 vol_pen=1.00 ret_pen=1.25\n",
      "[last180] clip@0=0.00%, clip@2=0.00%\n",
      "[inner] M_SOFT=1.421, T_SOFT=1.00, target clip≈22%\n",
      "[test ] clip@0=0.00% clip@2=0.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p style='color: orange'>train(:8630) test(8630:8810)<br>val_score:  1.128<br>score(submission): 0.119539<br>z-calib: b=4.762e-04, T=1.770e-02, M_SOFT=1.421, T_SOFT=1.00</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 2 Test start at 8450 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001443 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21575\n",
      "[LightGBM] [Info] Number of data points in the train set: 8450, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000453\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[211], line 12\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 単純なLightGBMモデルで試す\u001b[39;00m\n\u001b[1;32m      3\u001b[0m allocation_model \u001b[38;5;241m=\u001b[39m lgb\u001b[38;5;241m.\u001b[39mLGBMRegressor(\n\u001b[1;32m      4\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m      5\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.05\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      9\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     10\u001b[0m )\n\u001b[0;32m---> 12\u001b[0m submit_model \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallocation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mLightGBM Model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[210], line 67\u001b[0m, in \u001b[0;36mcross_validate\u001b[0;34m(allocation_model, label, min_train_size, test_size, inner_len, vol_mode, lambda_ewma, window_size, std_scale, T_soft_init, target_clip, conf_params, gamma, lev_cap)\u001b[0m\n\u001b[1;32m     61\u001b[0m sol_inner \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforward_returns\u001b[39m\u001b[38;5;124m\"\u001b[39m: y_all[inner_start:test_start],\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrisk_free_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m:  rfr_all[inner_start:test_start],\n\u001b[1;32m     64\u001b[0m })\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# ---- fit ----\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43mallocation_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_all\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mtest_start\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# ---- inner: z校正 & m_soft 決定 ----\u001b[39;00m\n\u001b[1;32m     70\u001b[0m y_pred_inner \u001b[38;5;241m=\u001b[39m allocation_model\u001b[38;5;241m.\u001b[39mpredict(X_inner)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:1092\u001b[0m, in \u001b[0;36mLGBMRegressor.fit\u001b[0;34m(self, X, y, sample_weight, init_score, eval_set, eval_names, eval_sample_weight, eval_init_score, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1077\u001b[0m     X: _LGBM_ScikitMatrixLike,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     init_model: Optional[Union[\u001b[38;5;28mstr\u001b[39m, Path, Booster, LGBMModel]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLGBMRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Docstring is inherited from the LGBMModel.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1092\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1094\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1096\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1097\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1098\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1099\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_sample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_init_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_init_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1101\u001b[0m \u001b[43m        \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcategorical_feature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcategorical_feature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1105\u001b[0m \u001b[43m        \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/sklearn.py:885\u001b[0m, in \u001b[0;36mLGBMModel.fit\u001b[0;34m(self, X, y, sample_weight, init_score, group, eval_set, eval_names, eval_sample_weight, eval_class_weight, eval_init_score, eval_group, eval_metric, feature_name, categorical_feature, callbacks, init_model)\u001b[0m\n\u001b[1;32m    882\u001b[0m evals_result: _EvalResultDict \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    883\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mappend(record_evaluation(evals_result))\n\u001b[0;32m--> 885\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_boost_round\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_sets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_sets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalid_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_metrics_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evals_result \u001b[38;5;241m=\u001b[39m evals_result\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_best_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster\u001b[38;5;241m.\u001b[39mbest_iteration\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/engine.py:276\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, train_set, num_boost_round, valid_sets, valid_names, feval, init_model, feature_name, categorical_feature, keep_training_booster, callbacks)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m cb \u001b[38;5;129;01min\u001b[39;00m callbacks_before_iter:\n\u001b[1;32m    269\u001b[0m     cb(callback\u001b[38;5;241m.\u001b[39mCallbackEnv(model\u001b[38;5;241m=\u001b[39mbooster,\n\u001b[1;32m    270\u001b[0m                             params\u001b[38;5;241m=\u001b[39mparams,\n\u001b[1;32m    271\u001b[0m                             iteration\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m    272\u001b[0m                             begin_iteration\u001b[38;5;241m=\u001b[39minit_iteration,\n\u001b[1;32m    273\u001b[0m                             end_iteration\u001b[38;5;241m=\u001b[39minit_iteration \u001b[38;5;241m+\u001b[39m num_boost_round,\n\u001b[1;32m    274\u001b[0m                             evaluation_result_list\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m--> 276\u001b[0m \u001b[43mbooster\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m evaluation_result_list: List[_LGBM_BoosterEvalMethodResultType] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m# check evaluation result.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/lightgbm/basic.py:3891\u001b[0m, in \u001b[0;36mBooster.update\u001b[0;34m(self, train_set, fobj)\u001b[0m\n\u001b[1;32m   3889\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__set_objective_to_none:\n\u001b[1;32m   3890\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LightGBMError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot update due to null objective function.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 3891\u001b[0m _safe_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLGBM_BoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3892\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbyref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_finished\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   3894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__is_predicted_cur_iter \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__num_dataset)]\n\u001b[1;32m   3895\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_finished\u001b[38;5;241m.\u001b[39mvalue \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 単純なLightGBMモデルで試す\n",
    "\n",
    "allocation_model = lgb.LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "submit_model = cross_validate(allocation_model, label=\"LightGBM Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c0cc0",
   "metadata": {},
   "source": [
    "### Submission\n",
    "- time-series streaming形式\n",
    "- Kaggle サーバーから1batchずつ送られるデータからsubmission.parquetを返す\n",
    "- 返り値検証があるため，指定された形式で返す\n",
    "- 指定形式\n",
    "  - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218685fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"Replace this function with your inference code.\"\"\"\n",
    "    test_pd = test.to_pandas()\n",
    "    # display(test_pd.info())\n",
    "    if len(test_pd.columns) > 94:\n",
    "        test_pd = test_pd.drop(\n",
    "            [\"date_id\", \"is_scored\", \"lagged_forward_returns\", \"lagged_risk_free_rate\", \"lagged_market_forward_excess_returns\"], \n",
    "            axis = 1)\n",
    "    \n",
    "    preds = submit_model.predict(test_pd)\n",
    "    raw_pred: float = float(preds[0])\n",
    "    print(f\"predict:{raw_pred}\")\n",
    "    \n",
    "    # --- 出力（float or ndarray）---\n",
    "    # KaggleのAPI仕様上、float単体かSeries/DataFrameで返す必要あり float(preds[0]) if len(preds) == 1 else preds　\n",
    "    return convert_ret_to_signal(raw_pred, ret_signal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05b6a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "GatewayRuntimeError",
     "evalue": "(<GatewayRuntimeErrorType.GATEWAY_RAISED_EXCEPTION: 5>, 'Traceback (most recent call last):\\n  File \"/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/base_gateway.py\", line 134, in run\\n    predictions, row_ids = self.get_all_predictions()\\n  File \"/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/base_gateway.py\", line 109, in get_all_predictions\\n    for data_batch, row_ids in self.generate_data_batches():\\n  File \"/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/default_gateway.py\", line 29, in generate_data_batches\\n    test = pl.read_csv(self.competition_data_dir / \\'test.csv\\')\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/_utils/deprecation.py\", line 128, in wrapper\\n    return function(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/_utils/deprecation.py\", line 128, in wrapper\\n    return function(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/_utils/deprecation.py\", line 128, in wrapper\\n    return function(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/io/csv/functions.py\", line 549, in read_csv\\n    df = _read_csv_impl(\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/io/csv/functions.py\", line 697, in _read_csv_impl\\n    pydf = PyDataFrame.read_csv(\\nFileNotFoundError: No such file or directory (os error 2): /kaggle/input/hull-tactical-market-prediction/test.csv\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGatewayRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[201], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     inference_server\u001b[38;5;241m.\u001b[39mserve()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     \u001b[43minference_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_local_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/hull-tactical-market-prediction/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/templates.py:110\u001b[0m, in \u001b[0;36mInferenceServer.run_local_gateway\u001b[0;34m(self, data_paths, file_share_dir, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39mstop(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/templates.py:108\u001b[0m, in \u001b[0;36mInferenceServer.run_local_gateway\u001b[0;34m(self, data_paths, file_share_dir, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_gateway_for_test(data_paths, file_share_dir, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/base_gateway.py:153\u001b[0m, in \u001b[0;36mBaseGateway.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_result(error)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# For local testing\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[0;31mGatewayRuntimeError\u001b[0m: (<GatewayRuntimeErrorType.GATEWAY_RAISED_EXCEPTION: 5>, 'Traceback (most recent call last):\\n  File \"/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/base_gateway.py\", line 134, in run\\n    predictions, row_ids = self.get_all_predictions()\\n  File \"/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/base_gateway.py\", line 109, in get_all_predictions\\n    for data_batch, row_ids in self.generate_data_batches():\\n  File \"/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/default_gateway.py\", line 29, in generate_data_batches\\n    test = pl.read_csv(self.competition_data_dir / \\'test.csv\\')\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/_utils/deprecation.py\", line 128, in wrapper\\n    return function(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/_utils/deprecation.py\", line 128, in wrapper\\n    return function(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/_utils/deprecation.py\", line 128, in wrapper\\n    return function(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/io/csv/functions.py\", line 549, in read_csv\\n    df = _read_csv_impl(\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/io/csv/functions.py\", line 697, in _read_csv_impl\\n    pydf = PyDataFrame.read_csv(\\nFileNotFoundError: No such file or directory (os error 2): /kaggle/input/hull-tactical-market-prediction/test.csv\\n')"
     ]
    }
   ],
   "source": [
    "# サーバー上でpredict(test_batch)を動かす\n",
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
