{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da04843",
   "metadata": {},
   "source": [
    "# Hull Tactical Market Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2358dfe",
   "metadata": {},
   "source": [
    "### Import Libralies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "33e2c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "# Models\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Submission\n",
    "import polars as pl\n",
    "import kaggle_evaluation.default_inference_server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7e4c475b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "427f4438",
   "metadata": {},
   "outputs": [],
   "source": [
    "INNER_VAL_LEN = 180\n",
    "TRADING_DAYS_PER_YR = 252"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad54b45d",
   "metadata": {},
   "source": [
    "### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b41dbf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ RETURNS TO SIGNAL CONFIGS ============\n",
    "MIN_SIGNAL: float = 0.0                         # Minimum value for the daily signal \n",
    "MAX_SIGNAL: float = 2.0                         # Maximum value for the daily signal \n",
    "SIGNAL_MULTIPLIER: float = 7.5                 # Multiplier of the OLS market forward excess returns predictions to signal \n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class RetToSignalParameters:\n",
    "    signal_multiplier: float \n",
    "    min_signal : float = MIN_SIGNAL\n",
    "    max_signal : float = MAX_SIGNAL\n",
    "\n",
    "ret_signal_params = RetToSignalParameters(\n",
    "    signal_multiplier= SIGNAL_MULTIPLIER\n",
    ")\n",
    "\n",
    "def convert_ret_to_signal(\n",
    "    ret_arr: np.ndarray,\n",
    "    params: RetToSignalParameters,\n",
    "    signal_multiplier=None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Converts raw model predictions (expected returns) into a trading signal.\n",
    "\n",
    "    Args:\n",
    "        ret_arr (np.ndarray): The array of predicted returns.\n",
    "        params (RetToSignalParameters): Parameters for scaling and clipping the signal.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The resulting trading signal, clipped between min and max values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 予測値を基準に，投資戦略シグナルに変換\n",
    "    # ret * signal_multiplier + 1 を min_signal ~ max_signal の範囲にクリップ\n",
    "    if signal_multiplier is None:\n",
    "        multi = params.signal_multiplier    \n",
    "    else:\n",
    "        multi = signal_multiplier\n",
    "    \n",
    "    ret = np.clip(\n",
    "        ret_arr * multi + 1,\n",
    "        params.min_signal, \n",
    "        params.max_signal\n",
    "    )\n",
    "\n",
    "    if ret.size < 20:\n",
    "        print(f\"Strategy:\")\n",
    "        for i, value in enumerate(ret): print(f'  {i}: {value:.4f}')\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a444b812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy:\n",
      "  0: 2.0000\n",
      "  1: 1.7500\n",
      "  2: 2.0000\n",
      "  3: 0.0000\n",
      "  4: 2.0000\n"
     ]
    }
   ],
   "source": [
    "# convert_ret_to_signalの動作確認\n",
    "# 20個の乱数(0~1)\n",
    "hoge = convert_ret_to_signal(np.array([5, 0.1, 0.3, -0.2, 1.3]), ret_signal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dfc2f554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ LOAD DATA ============\n",
    "# プラットフォームがkaggleかローカルかで分岐\n",
    "if os.getenv('KAGGLE_KERNEL_RUN_TYPE') is not None:\n",
    "    # Kaggle上\n",
    "    DATA_PATH: Path = Path('/kaggle/input/hull-tactical-market-prediction/')\n",
    "else:\n",
    "    BASE_PATH = Path.cwd()\n",
    "    DATA_PATH: Path = BASE_PATH / 'data'\n",
    "\n",
    "\n",
    "train = pd.read_csv(DATA_PATH / \"train.csv\")\n",
    "test = pd.read_csv(DATA_PATH / \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ef6c12",
   "metadata": {},
   "source": [
    "### Scoreing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "862dff85",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticipantVisibleError(Exception):\n",
    "    # Custom error to show messages to participants\n",
    "    pass\n",
    "\n",
    "def score(solution: pd.DataFrame, submission: pd.DataFrame, row_id_column_name: str, intermediate_res:list = []) -> float:\n",
    "    \"\"\"\n",
    "    Calculates a custom evaluation metric (volatility-adjusted Sharpe ratio).\n",
    "\n",
    "    This metric penalizes strategies that take on significantly more volatility\n",
    "    than the underlying market.\n",
    "\n",
    "    Returns:\n",
    "        float: The calculated adjusted Sharpe ratio.\n",
    "    \"\"\"\n",
    "    solution = solution.copy().reset_index(drop=True)\n",
    "    submission = submission.copy().reset_index(drop=True)\n",
    "    solution['position'] = submission['prediction']\n",
    "\n",
    "    # ありえない値を除外する (0 <= position <= 2)\n",
    "        # 0 means that we don't invest in S & P at all but get only the risk-free rate.\n",
    "        # 1 means that we invest all our money in S & P.\n",
    "        # 2 means that we invest twice our capital in S & P while taking a credit at the risk-free rate.\n",
    "        # -> つまり，普通に預金するか，S&Pに投資するか，S&Pに2倍レバレッジで投資するか（借金）の割合\n",
    "    if solution['position'].max() > MAX_SIGNAL:\n",
    "        raise ParticipantVisibleError(f'Position of {solution[\"position\"].max()} exceeds maximum of {MAX_SIGNAL}')\n",
    "    if solution['position'].min() < MIN_SIGNAL:\n",
    "        raise ParticipantVisibleError(f'Position of {solution[\"position\"].min()} below minimum of {MIN_SIGNAL}')\n",
    "\n",
    "    # Calculate strategy returns\n",
    "    # フェデラルファンド金利(利息) * (1-予測値) + 予測値 * S&P500の翌日のリターン = 戦略のリターン(割合)\n",
    "    solution['strategy_returns'] = solution['risk_free_rate'] * (1 - solution['position']) + solution['position'] * solution['forward_returns']\n",
    "\n",
    "    # Calculate strategy's Sharpe ratio\n",
    "    # リターンとその標準偏差を用いてシャープレシオ（リスクあたりの効率）を計算\n",
    "    strategy_excess_returns = solution['strategy_returns'] - solution['risk_free_rate'] # 超過リターン -> 今回の戦略で得た割合から，リスクフリー時の割合を引いた分\n",
    "    strategy_excess_cumulative = (1 + strategy_excess_returns).prod() # 累積超過リターン -> 全期間の超過リターンをかけ合わせた分(1+で倍率に変換)\n",
    "    strategy_mean_excess_return = (strategy_excess_cumulative) ** (1 / len(solution)) - 1 # 平均超過リターン -> 複利は幾何平均で求める． また，倍率から割合に戻してる\n",
    "    strategy_std = solution['strategy_returns'].std() # リターンの標準偏差\n",
    "\n",
    "    trading_days_per_yr = 252 # 1年あたりの取引日数(固定値)\n",
    "    if strategy_std == 0:\n",
    "        raise ZeroDivisionError\n",
    "    sharpe = strategy_mean_excess_return / strategy_std * np.sqrt(trading_days_per_yr) # 年率換算したシャープレシオ. sqrt(252)をかけることで年率換算している（統計的な性質らしい）\n",
    "    strategy_volatility = float(strategy_std * np.sqrt(trading_days_per_yr) * 100)  # 年率換算したボラティリティ(価格変動率)\n",
    "\n",
    "    # Calculate market return and volatility\n",
    "    # S&P500に投資し続けた場合のリターンとボラティリティを計算\n",
    "    market_excess_returns = solution['forward_returns'] - solution['risk_free_rate'] # S&P500が利息を上回る割合\n",
    "    market_excess_cumulative = (1 + market_excess_returns).prod() # ↑の累積\n",
    "    market_mean_excess_return = (market_excess_cumulative) ** (1 / len(solution)) - 1 # train: 0.0003066067595838273 幾何平均，割合化\n",
    "    market_std = solution['forward_returns'].std() # S&P500のリターンの標準偏差\n",
    "    \n",
    "    market_volatility = float(market_std * np.sqrt(trading_days_per_yr) * 100) # train: 16.748459963166347 %\n",
    "    \n",
    "    # Calculate the volatility penalty\n",
    "    # ボラティリティペナルティを計算\n",
    "    # -> 市場のボラティリティの1.2倍を超える場合のペナルティ\n",
    "    excess_vol = max(0, strategy_volatility / market_volatility - 1.2) if market_volatility > 0 else 0\n",
    "    vol_penalty = 1 + excess_vol\n",
    "\n",
    "    # Calculate the return penalty\n",
    "    # リターンペナルティを計算\n",
    "    # -> 市場のリターンを下回る場合のペナルティ\n",
    "    return_gap = max(\n",
    "        0,\n",
    "        (market_mean_excess_return - strategy_mean_excess_return) * 100 * trading_days_per_yr,\n",
    "    )\n",
    "    return_penalty = 1 + (return_gap**2) / 100\n",
    "\n",
    "    # Adjust the Sharpe ratio by the volatility and return penalty\n",
    "    # ペナルティ値の反映\n",
    "    adjusted_sharpe = sharpe / (vol_penalty * return_penalty)\n",
    "\n",
    "    # print(\"strategy_excess_returns NaN数:\", solution['strategy_returns'].isna().sum())\n",
    "    # print(\"strategy_std:\", strategy_std)\n",
    "    # print(\"strategy_excess_cumulative:\", strategy_excess_cumulative)\n",
    "    # print(\"market_excess_cumulative:\", market_excess_cumulative)\n",
    "    # print(\"adjusted_sharpe:\", adjusted_sharpe)\n",
    "    try:\n",
    "        intermediate_res.append((strategy_mean_excess_return, strategy_std, sharpe, vol_penalty, return_penalty)) # 各値を記録(debug)\n",
    "        return min(float(adjusted_sharpe), 1_000_000), intermediate_res # float変換，上限100万\n",
    "    except NameError:\n",
    "        return min(float(adjusted_sharpe), 1_000_000) # float変換，上限100万"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836dbfa5",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19f11e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d54d97c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rolling_vol_no_leak(fr: pd.Series, window_size: int) -> np.ndarray:\n",
    "    # 過去のみ参照（center=False）。窓不足の序盤は expanding std で埋める（未来は使わない）\n",
    "    roll = fr.rolling(window=window_size, min_periods=window_size, center=False).std()\n",
    "    # 序盤: 窓不足部分（NaN）を expanding std で補う（過去のみ）\n",
    "    expd = fr.expanding(min_periods=2).std()  # 1点ではstdはNaNのまま\n",
    "    vol = roll.combine_first(expd).bfill(limit=0)  # bfill(limit=0) 実質何もしない（安全のため残す）\n",
    "    return vol.to_numpy()\n",
    "\n",
    "def _ewma_vol_series(fr: pd.Series, lam: float = 0.94, min_periods: int = 2) -> np.ndarray:\n",
    "    # EWMA分散 → 平方根でボラ。adjust=False で再帰形（オンライン更新と等価）\n",
    "    var = (fr**2).ewm(alpha=1 - lam, adjust=False, min_periods=min_periods).mean()\n",
    "    return np.sqrt(var).to_numpy()\n",
    "\n",
    "def calculate_volatility_scaling_factor(\n",
    "    y_pred: np.ndarray,\n",
    "    solution: pd.DataFrame,\n",
    "    window_size: int = 30,\n",
    "    *,\n",
    "    vol_mode: str = \"ewma\",         # \"rolling\" or \"ewma\"\n",
    "    lambda_ewma: float = 0.94,\n",
    "    k_grid: np.ndarray | None = None,\n",
    "    alpha_grid: np.ndarray | None = None,\n",
    "    eps: float = 1e-6,\n",
    "    lambda_reg: float = 0.0\n",
    ") -> tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    ボラ依存スケーリング:\n",
    "        m_t = k / (sigma_t + eps)^alpha\n",
    "    を inner データ上で score() 最大化となる (k, alpha) を探索。\n",
    "\n",
    "    Args:\n",
    "        y_pred: inner 区間のモデル予測（len == len(solution)）\n",
    "        solution: 'forward_returns', 'risk_free_rate' を持つ DataFrame（inner 区間）\n",
    "        window_size: rolling 用の窓\n",
    "        vol_mode: \"rolling\" or \"ewma\"\n",
    "        lambda_ewma: EWMA の λ\n",
    "        k_grid, alpha_grid: 探索グリッド（未指定なら内部既定）\n",
    "        eps: ゼロ除算回避\n",
    "        lambda_reg: L2 正則化強度\n",
    "\n",
    "    Returns:\n",
    "        best_k (float)     :  最適化された k の値\n",
    "        best_alpha (float) :  最適化された α の値\n",
    "        best_score (float) :  最適化されたときの報酬値\n",
    "    \"\"\"\n",
    "    # 入力検証\n",
    "    y_pred = np.asarray(y_pred).reshape(-1)\n",
    "    if not {\"forward_returns\", \"risk_free_rate\"}.issubset(solution.columns):\n",
    "        raise ValueError(\"solution must have columns: forward_returns, risk_free_rate\")\n",
    "    if len(y_pred) != len(solution):\n",
    "        raise ValueError(f\"Length mismatch: y_pred={len(y_pred)} vs solution={len(solution)}\")\n",
    "\n",
    "    fr = solution[\"forward_returns\"]\n",
    "\n",
    "    # ボラ計算\n",
    "    if vol_mode == \"rolling\":\n",
    "        market_vol = _rolling_vol_no_leak(fr, window_size=window_size)\n",
    "    elif vol_mode == \"ewma\":\n",
    "        market_vol = _ewma_vol_series(fr, lam=lambda_ewma, min_periods=max(2, int(window_size // 4)))\n",
    "    else:\n",
    "        raise ValueError(\"vol_mode must be 'rolling' or 'ewma'\")\n",
    "\n",
    "    # グリッド定義\n",
    "    if k_grid is None:\n",
    "        k_grid = np.logspace(-4, np.log10(1.0), 20)  # kの探索範囲:10^-4 ~ 1.0までを20ステップ\n",
    "    if alpha_grid is None:\n",
    "        alpha_grid = np.linspace(0, 1.0, 5)  # αの探索範囲とステップ\n",
    "\n",
    "\n",
    "    best_score = -np.inf\n",
    "    best_k = None\n",
    "    best_alpha = None\n",
    "\n",
    "    for k in k_grid:\n",
    "        for alpha in alpha_grid:\n",
    "            m_t = k / np.power(market_vol + eps, alpha)\n",
    "            # 戦略(0-2)に変換：convert_ret_to_signal がベクトルmultiplier非対応なら、先に y_pred*m_t を計算して渡す\n",
    "            alloc = convert_ret_to_signal(y_pred, ret_signal_params, signal_multiplier=m_t)\n",
    "            sub = pd.DataFrame({\"prediction\": alloc}).reset_index(drop=True)\n",
    "            s, _ = score(solution, sub, \"\", [])\n",
    "            if lambda_reg > 0:\n",
    "                s -= lambda_reg * (k**2 + alpha**2)\n",
    "            if s > best_score:\n",
    "                best_score, best_k, best_alpha = s, float(k), float(alpha)\n",
    "                # print(f\"New best score: {best_score:.6f} (k={best_k:.6f}, alpha={best_alpha:.6f})\")\n",
    "\n",
    "    return float(best_k), float(best_alpha), float(best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "9cb9f865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_sigma_for_period(fr_all: np.ndarray, start: int, end: int,\n",
    "                           mode=\"ewma\", lam=0.94, window=30) -> np.ndarray:\n",
    "    \"\"\" [start, end) 用の σ_t を、直前の過去だけを使って作る（リークなし）。 \"\"\"\n",
    "    warm = max(window, 20)\n",
    "    prefix_start = max(0, start - warm)\n",
    "    fr_prefix = fr_all[prefix_start:start]\n",
    "    fr_period  = fr_all[start:end]\n",
    "    fr_concat  = np.concatenate([fr_prefix, fr_period])\n",
    "    s = pd.Series(fr_concat)\n",
    "    if mode == \"rolling\":\n",
    "        sigma_all = _rolling_vol_no_leak(s, window_size=window)\n",
    "    elif mode == \"ewma\":\n",
    "        sigma_all = _ewma_vol_series(s, lam=lam, min_periods=max(2, window//4))\n",
    "    else:\n",
    "        raise ValueError(\"vol_mode must be 'rolling' or 'ewma'\")\n",
    "    return sigma_all[-(end - start):]  # 期間ぶんだけ取り出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4d1585ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_list_dict = {}\n",
    "def cross_validate(allocation_model, label=\"\", min_train_size=1500, test_size=180):\n",
    "    \"\"\"\n",
    "    時系列を考慮した交差検証を行う\n",
    "    検証インデックス：\n",
    "    range(len(train) - test_size, min_train_size, - test_size)\n",
    "    例えば、trainデータが2000行、test_size=120、min_train_size=1500の場合、\n",
    "    検証用のインデックスは：\n",
    "    range(2000 - 120, 1500, -120) = range(1880, 1500, -120)\n",
    "    = (1880, 1760, 1640, 1520)\n",
    "    となり、各foldで未来のデータを使用せずに評価することができる．\n",
    "    trainサイズはfoldが進む毎に減少し，min_train_sizeに達したら終了する．\n",
    "    減少させているのは未来リークを防ぐため．\n",
    "    \"\"\"\n",
    "    n = len(train)\n",
    "    oof = np.full(n, np.nan, dtype=float)\n",
    "    score_list = []\n",
    "    intermediate_res = []\n",
    "    val_list = []\n",
    "\n",
    "    # ===== 前処理（1回だけ） =====\n",
    "    # 学習に使わない列を除いた特徴量列を一度だけ確定\n",
    "    drop_cols = [\"date_id\", \"forward_returns\", \"risk_free_rate\", \"market_forward_excess_returns\"]\n",
    "    feature_cols = [c for c in train.columns if c not in drop_cols]\n",
    "\n",
    "    # 必要列を配列/ビューで保持（コピー最小化）\n",
    "    X_all = train[feature_cols]                  # DataFrame（ループ内は iloc でビュー切り）\n",
    "    y_all = train[\"forward_returns\"].to_numpy()  # 1D ndarray\n",
    "    rfr_all = train[\"risk_free_rate\"].to_numpy() # 1D ndarray\n",
    "\n",
    "    # バリデーション（固定で最後180）を一度だけ切り出し\n",
    "    val_idx_start = max(0, n - 180)\n",
    "    X_val = X_all.iloc[val_idx_start:]\n",
    "    y_val = y_all[val_idx_start:]\n",
    "    v_sol = pd.DataFrame(\n",
    "        {\n",
    "            \"forward_returns\": y_all[val_idx_start:],\n",
    "            \"risk_free_rate\":  rfr_all[val_idx_start:],\n",
    "        }\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    for fold, test_start in enumerate(\n",
    "        range(n - test_size, min_train_size, -test_size)\n",
    "    ):\n",
    "        print(Fore.CYAN + f\"=== Fold {fold} Test start at {test_start} ===\" + Style.RESET_ALL)\n",
    "        # 1. データ分割（中間DataFrameを作らず直接切る）\n",
    "        test_end = test_start + test_size\n",
    "\n",
    "        X_train = X_all.iloc[:test_start]\n",
    "        y_train = y_all[:test_start]\n",
    "\n",
    "        X_test = X_all.iloc[test_start:test_end]\n",
    "        y_test = y_all[test_start:test_end]  # 使わないが念のため残す（デバッグ等）\n",
    "        \n",
    "        # 評価用（score用のソリューション; 必要最小限のDF生成）\n",
    "        solution = pd.DataFrame(\n",
    "            {\n",
    "                \"forward_returns\": y_all[test_start:test_end],\n",
    "                \"risk_free_rate\":  rfr_all[test_start:test_end],\n",
    "            }\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        # 1.5. SIGNALE_MULTIPLIER の最適化\n",
    "        # --- inner validation を train_fold の末尾から作る ---\n",
    "        inner_start = max(0, test_start - INNER_VAL_LEN)\n",
    "        X_inner = X_all.iloc[inner_start:test_start]\n",
    "        y_inner = y_all[inner_start:test_start]\n",
    "        sol_inner = pd.DataFrame({\n",
    "            \"forward_returns\": y_all[inner_start:test_start],\n",
    "            \"risk_free_rate\":  rfr_all[inner_start:test_start],\n",
    "        }).reset_index(drop=True)\n",
    "\n",
    "        # 2. モデル学習\n",
    "        allocation_model.fit(X_train, y_train)\n",
    "\n",
    "        # 2.5. inner validation で (k, alpha) を決定\n",
    "        y_pred_inner = allocation_model.predict(X_inner)\n",
    "        best_k, best_alpha, best_score = calculate_volatility_scaling_factor(y_pred_inner, sol_inner)\n",
    "\n",
    "        # ===== テスト区間の m_t を “再計算” して適用（リークなし） =====\n",
    "        # vol の作り方は calculate_volatility_scaling_factor と同じモード/パラメータに揃える\n",
    "        VOL_MODE = \"ewma\"       # or \"rolling\"\n",
    "        LAMBDA_EWMA = 0.94\n",
    "        WINDOW_SIZE = 30\n",
    "        EPS = 1e-6\n",
    "\n",
    "        # 3. 予測\n",
    "        y_pred = allocation_model.predict(X_test)\n",
    "\n",
    "        # テスト用 σ_t と m_t（ベクトル）を作成\n",
    "        sigma_test = _make_sigma_for_period(y_all, test_start, test_end,\n",
    "                                            mode=VOL_MODE, lam=LAMBDA_EWMA, window=WINDOW_SIZE)\n",
    "        m_t_test = best_k / np.power(sigma_test + EPS, best_alpha)\n",
    "        # allocation_list = np.clip(y_pred, 0, 2)  # 投資比率は0から2の間にクリップ\n",
    "        allocation_list = convert_ret_to_signal(y_pred, ret_signal_params, signal_multiplier=m_t_test)\n",
    "\n",
    "        # 4. 評価\n",
    "        submission = pd.DataFrame({\"prediction\": allocation_list}).reset_index(drop=True)\n",
    "        validation_score, intermediate_res = score(\n",
    "            solution, submission, \"\", intermediate_res\n",
    "        )\n",
    "\n",
    "        pred_val = allocation_model.predict(X_val)\n",
    "        pred_val = allocation_model.predict(X_val)\n",
    "        val_start = val_idx_start\n",
    "        val_end   = n\n",
    "        sigma_val = _make_sigma_for_period(y_all, val_start, val_end,\n",
    "                                        mode=VOL_MODE, lam=LAMBDA_EWMA, window=WINDOW_SIZE)\n",
    "        m_t_val = best_k / np.power(sigma_val + EPS, best_alpha)\n",
    "\n",
    "        val_allocation_list = convert_ret_to_signal(pred_val, ret_signal_params, signal_multiplier=m_t_val)\n",
    "        val_submission = pd.DataFrame({\"prediction\": val_allocation_list}).reset_index(drop=True)\n",
    "        val_score, inter = score(\n",
    "            v_sol, val_submission, \"\", intermediate_res\n",
    "        )\n",
    "        if inter:\n",
    "            strat_mu, strat_std, sharpe, vol_pen, ret_pen = inter[-1]\n",
    "            print(f\"[last180] sharpe={sharpe:.3f} vol_pen={vol_pen:.2f} ret_pen={ret_pen:.2f}\")\n",
    "        lo = np.mean(val_allocation_list <= 0.0)\n",
    "        hi = np.mean(val_allocation_list >= 2.0)\n",
    "        print(f\"[last180] clip@0={lo:.2%}, clip@2={hi:.2%}\")\n",
    "        \n",
    "        vol_penalty = intermediate_res[-1][3]   # ボラティリティペナルティ\n",
    "        return_penalty = intermediate_res[-1][4]# リターンペナルティ\n",
    "        \n",
    "        display(HTML(\n",
    "            f\"<p  style='color: orange'>\"\n",
    "            f\"train(:{test_start:4}) test({test_start:4}:{test_end:4})<br>\"\n",
    "            f\"val_score: {validation_score:6.3f} {vol_penalty=:.2f} {return_penalty=:.2f}<br>\"\n",
    "            f\"score(submission) : {val_score}<br>\"\n",
    "            f\"best_k={best_k:.3f}, best_alpha={best_alpha:.3f}, \"\n",
    "            f\"mean(m_test)={float(np.mean(m_t_test)):.3f}, mean(m_val)={float(np.mean(m_t_val)):.3f}\"\n",
    "            f\"</p>\"\n",
    "        ))\n",
    "        \n",
    "        oof[test_start:test_end] = allocation_list\n",
    "        score_list.append(validation_score)\n",
    "        val_list.append(val_score)\n",
    "\n",
    "        # 最初のfold modelを保存しておく\n",
    "        # if fold == 0:\n",
    "        #     submit_model = allocation_model\n",
    "        # else :\n",
    "        #     break\n",
    "\n",
    "    # ===== 集計表示 =====\n",
    "    submit_model = allocation_model\n",
    "    display(HTML('<h2 style=\"text-align:center;color:orange\">======== Result ========</h2>'))\n",
    "    avg_validation_score = float(np.nanmean(score_list)) if len(score_list) else np.nan\n",
    "    print(f\"{label} Average Validation Score: {avg_validation_score:.6f}\")\n",
    "    \n",
    "    # 全体スコア（インデックス揃え）\n",
    "    mask = np.isfinite(oof)\n",
    "    if np.any(mask):\n",
    "        solution_all = pd.DataFrame(\n",
    "            {\n",
    "                \"forward_returns\": y_all[mask],\n",
    "                \"risk_free_rate\":  rfr_all[mask],\n",
    "            }\n",
    "        ).reset_index(drop=True)\n",
    "        submission_all = pd.DataFrame({'prediction': oof[mask]}).reset_index(drop=True)\n",
    "        overall_score, intermediate_res = score(solution_all, submission_all, '', intermediate_res)\n",
    "        vol_penalty = intermediate_res[-1][3] if intermediate_res else np.nan\n",
    "        return_penalty = intermediate_res[-1][4] if intermediate_res else np.nan\n",
    "        print(f\"{label} Overall Validation Score: {overall_score:.6f} vol_penalty={vol_penalty:.2f} return_penalty={return_penalty:.2f}\")\n",
    "    else:\n",
    "        print(f\"{label} Overall Validation Score: NaN (no valid OOF)\")\n",
    "\n",
    "\n",
    "\n",
    "    score_list_dict[label] = score_list\n",
    "    # 1回目のfoldのスコアを示す\n",
    "    if score_list:\n",
    "        print(f\"{label} First(Test) Fold Validation Score: {score_list[0]:.6f}\")\n",
    "\n",
    "    if val_list:\n",
    "        print(Fore.YELLOW + f\"All(Test) Fold Validation Score : {(sum(val_list) / len(val_list)):6.3f}\" + Style.RESET_ALL)\n",
    "        \n",
    "\n",
    "    # 分布可視化\n",
    "    vals = oof[mask]\n",
    "    if len(vals):\n",
    "        vmin, vmax = float(np.min(vals)), float(np.max(vals))\n",
    "        if vmin == vmax:\n",
    "            vmax = vmin + 1e-6\n",
    "        bins = np.linspace(vmin, vmax, 50)\n",
    "        plt.figure(figsize=(6, 2))\n",
    "        plt.hist(vals, bins=bins, density=False, color='c', edgecolor='k', linewidth=0.5)\n",
    "        plt.title(f'Allocation histogram of {label}')\n",
    "        plt.gca().get_yaxis().set_visible(False)\n",
    "        plt.xlim(vmin, vmax)\n",
    "        plt.show()\n",
    "\n",
    "    print(f\"Range of predictions: [{vmin:.6f}, {vmax:.6f}]\")\n",
    "    \n",
    "    # SIGNAL_MULTIPLIER の算出（元ロジック踏襲）\n",
    "    # ※ oofの分布に依存するため、maskチェックを入れる\n",
    "    if np.any(mask):\n",
    "        span = min(MAX_SIGNAL - 1.0, 1.0 - MIN_SIGNAL)  # 0-2なら span=1.0\n",
    "        q = np.percentile(np.abs(oof[mask]), 99)        # 上位1%に合わせる\n",
    "        SIGNAL_MULTIPLIER = (0.95 * span) / max(q, 1e-12)\n",
    "        print(f\"multi::{SIGNAL_MULTIPLIER}\")\n",
    "    else:\n",
    "        print(\"multi::NaN (no valid OOF)\")\n",
    "    \n",
    "\n",
    "    return submit_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "a3ca9afa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 0 Test start at 8810 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001665 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21575\n",
      "[LightGBM] [Info] Number of data points in the train set: 8810, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000468\n",
      "[last180] sharpe=0.349 vol_pen=1.12 ret_pen=1.00\n",
      "[last180] clip@0=0.00%, clip@2=1.11%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:8810) test(8810:8990)<br>val_score:  0.312 vol_penalty=1.12 return_penalty=1.00<br>score(submission) : 0.3115913993209355<br>best_k=1.000, best_alpha=1.000, mean(m_test)=108.664, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 1 Test start at 8630 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002010 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21573\n",
      "[LightGBM] [Info] Number of data points in the train set: 8630, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000460\n",
      "[last180] sharpe=0.134 vol_pen=1.08 ret_pen=1.25\n",
      "[last180] clip@0=0.00%, clip@2=0.56%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:8630) test(8630:8810)<br>val_score:  1.088 vol_penalty=1.08 return_penalty=1.25<br>score(submission) : 0.09912065887810453<br>best_k=1.000, best_alpha=1.000, mean(m_test)=136.037, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 2 Test start at 8450 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001952 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21575\n",
      "[LightGBM] [Info] Number of data points in the train set: 8450, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000453\n",
      "[last180] sharpe=0.283 vol_pen=1.06 ret_pen=1.03\n",
      "[last180] clip@0=0.00%, clip@2=0.56%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:8450) test(8450:8630)<br>val_score:  1.309 vol_penalty=1.06 return_penalty=1.03<br>score(submission) : 0.2583627593508449<br>best_k=1.000, best_alpha=1.000, mean(m_test)=142.066, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 3 Test start at 8270 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001747 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21574\n",
      "[LightGBM] [Info] Number of data points in the train set: 8270, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000440\n",
      "[last180] sharpe=0.527 vol_pen=1.02 ret_pen=1.00\n",
      "[last180] clip@0=0.00%, clip@2=0.56%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:8270) test(8270:8450)<br>val_score:  1.395 vol_penalty=1.02 return_penalty=1.00<br>score(submission) : 0.5141772797549088<br>best_k=1.000, best_alpha=1.000, mean(m_test)=99.173, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 4 Test start at 8090 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002772 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21569\n",
      "[LightGBM] [Info] Number of data points in the train set: 8090, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000468\n",
      "[last180] sharpe=0.262 vol_pen=1.06 ret_pen=1.05\n",
      "[last180] clip@0=0.00%, clip@2=0.56%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:8090) test(8090:8270)<br>val_score: -0.582 vol_penalty=1.06 return_penalty=1.05<br>score(submission) : 0.2358496700284913<br>best_k=1.000, best_alpha=1.000, mean(m_test)=67.313, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 5 Test start at 7910 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002917 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21568\n",
      "[LightGBM] [Info] Number of data points in the train set: 7910, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000465\n",
      "[last180] sharpe=0.368 vol_pen=1.09 ret_pen=1.00\n",
      "[last180] clip@0=0.00%, clip@2=1.11%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:7910) test(7910:8090)<br>val_score:  1.719 vol_penalty=1.09 return_penalty=1.00<br>score(submission) : 0.3367294703116942<br>best_k=1.000, best_alpha=1.000, mean(m_test)=138.132, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 6 Test start at 7730 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001820 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21566\n",
      "[LightGBM] [Info] Number of data points in the train set: 7730, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000448\n",
      "[last180] sharpe=0.307 vol_pen=1.01 ret_pen=1.02\n",
      "[last180] clip@0=0.00%, clip@2=1.11%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:7730) test(7730:7910)<br>val_score:  0.737 vol_penalty=1.01 return_penalty=1.02<br>score(submission) : 0.29610587107654757<br>best_k=1.000, best_alpha=1.000, mean(m_test)=103.047, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 7 Test start at 7550 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001781 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21508\n",
      "[LightGBM] [Info] Number of data points in the train set: 7550, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000434\n",
      "[last180] sharpe=0.423 vol_pen=1.04 ret_pen=1.00\n",
      "[last180] clip@0=0.00%, clip@2=1.11%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:7550) test(7550:7730)<br>val_score:  0.417 vol_penalty=1.04 return_penalty=1.00<br>score(submission) : 0.40716877532066664<br>best_k=1.000, best_alpha=1.000, mean(m_test)=94.170, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 8 Test start at 7370 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002566 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21444\n",
      "[LightGBM] [Info] Number of data points in the train set: 7370, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000428\n",
      "[last180] sharpe=0.353 vol_pen=1.07 ret_pen=1.00\n",
      "[last180] clip@0=0.00%, clip@2=2.22%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:7370) test(7370:7550)<br>val_score:  1.188 vol_penalty=1.07 return_penalty=1.00<br>score(submission) : 0.3288743436626361<br>best_k=1.000, best_alpha=1.000, mean(m_test)=140.530, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 9 Test start at 7190 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001757 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21383\n",
      "[LightGBM] [Info] Number of data points in the train set: 7190, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000433\n",
      "[last180] sharpe=0.126 vol_pen=1.21 ret_pen=1.24\n",
      "[last180] clip@0=0.00%, clip@2=5.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:7190) test(7190:7370)<br>val_score:  0.234 vol_penalty=1.21 return_penalty=1.24<br>score(submission) : 0.08447569603434518<br>best_k=1.000, best_alpha=1.000, mean(m_test)=134.228, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 10 Test start at 7010 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001972 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21328\n",
      "[LightGBM] [Info] Number of data points in the train set: 7010, number of used features: 94\n",
      "[LightGBM] [Info] Start training from score 0.000428\n",
      "[last180] sharpe=0.404 vol_pen=1.11 ret_pen=1.00\n",
      "[last180] clip@0=0.00%, clip@2=3.89%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:7010) test(7010:7190)<br>val_score:  0.984 vol_penalty=1.11 return_penalty=1.00<br>score(submission) : 0.36538511195209433<br>best_k=1.000, best_alpha=1.000, mean(m_test)=165.681, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 11 Test start at 6830 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002110 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21313\n",
      "[LightGBM] [Info] Number of data points in the train set: 6830, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 0.000421\n",
      "[last180] sharpe=0.355 vol_pen=1.19 ret_pen=1.00\n",
      "[last180] clip@0=0.00%, clip@2=2.22%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:6830) test(6830:7010)<br>val_score:  1.547 vol_penalty=1.19 return_penalty=1.00<br>score(submission) : 0.29916573375208616<br>best_k=1.000, best_alpha=1.000, mean(m_test)=231.107, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 12 Test start at 6650 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21261\n",
      "[LightGBM] [Info] Number of data points in the train set: 6650, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 0.000413\n",
      "[last180] sharpe=0.229 vol_pen=1.25 ret_pen=1.05\n",
      "[last180] clip@0=0.00%, clip@2=4.44%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:6650) test(6650:6830)<br>val_score:  1.853 vol_penalty=1.25 return_penalty=1.05<br>score(submission) : 0.17496265775440348<br>best_k=1.000, best_alpha=1.000, mean(m_test)=164.640, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 13 Test start at 6470 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001722 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21194\n",
      "[LightGBM] [Info] Number of data points in the train set: 6470, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 0.000413\n",
      "[last180] sharpe=0.263 vol_pen=1.08 ret_pen=1.05\n",
      "[last180] clip@0=0.56%, clip@2=1.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:6470) test(6470:6650)<br>val_score:  1.378 vol_penalty=1.08 return_penalty=1.05<br>score(submission) : 0.23209054426741216<br>best_k=1.000, best_alpha=1.000, mean(m_test)=99.232, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 14 Test start at 6290 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001992 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 21072\n",
      "[LightGBM] [Info] Number of data points in the train set: 6290, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 0.000422\n",
      "[last180] sharpe=0.377 vol_pen=1.23 ret_pen=1.00\n",
      "[last180] clip@0=0.00%, clip@2=5.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:6290) test(6290:6470)<br>val_score:  0.327 vol_penalty=1.23 return_penalty=1.00<br>score(submission) : 0.306152114275405<br>best_k=1.000, best_alpha=1.000, mean(m_test)=131.053, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 15 Test start at 6110 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001737 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20660\n",
      "[LightGBM] [Info] Number of data points in the train set: 6110, number of used features: 93\n",
      "[LightGBM] [Info] Start training from score 0.000424\n",
      "[last180] sharpe=0.150 vol_pen=1.15 ret_pen=1.20\n",
      "[last180] clip@0=1.11%, clip@2=2.22%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:6110) test(6110:6290)<br>val_score:  1.930 vol_penalty=1.15 return_penalty=1.20<br>score(submission) : 0.10829257280177379<br>best_k=1.000, best_alpha=1.000, mean(m_test)=166.547, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 16 Test start at 5930 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002091 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 20389\n",
      "[LightGBM] [Info] Number of data points in the train set: 5930, number of used features: 92\n",
      "[LightGBM] [Info] Start training from score 0.000415\n",
      "[last180] sharpe=0.423 vol_pen=1.00 ret_pen=1.00\n",
      "[last180] clip@0=3.89%, clip@2=1.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:5930) test(5930:6110)<br>val_score:  1.710 vol_penalty=1.00 return_penalty=1.00<br>score(submission) : 0.42342566384658836<br>best_k=1.000, best_alpha=1.000, mean(m_test)=151.206, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 17 Test start at 5750 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002254 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19944\n",
      "[LightGBM] [Info] Number of data points in the train set: 5750, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 0.000396\n",
      "[last180] sharpe=0.329 vol_pen=1.02 ret_pen=1.01\n",
      "[last180] clip@0=0.56%, clip@2=1.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:5750) test(5750:5930)<br>val_score:  0.756 vol_penalty=1.02 return_penalty=1.01<br>score(submission) : 0.3185879268821681<br>best_k=1.000, best_alpha=1.000, mean(m_test)=134.024, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 18 Test start at 5570 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001512 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19631\n",
      "[LightGBM] [Info] Number of data points in the train set: 5570, number of used features: 91\n",
      "[LightGBM] [Info] Start training from score 0.000394\n",
      "[last180] sharpe=0.694 vol_pen=1.12 ret_pen=1.00\n",
      "[last180] clip@0=0.00%, clip@2=2.78%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:5570) test(5570:5750)<br>val_score:  0.799 vol_penalty=1.12 return_penalty=1.00<br>score(submission) : 0.6209706451339749<br>best_k=1.000, best_alpha=1.000, mean(m_test)=129.833, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 19 Test start at 5390 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001608 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19296\n",
      "[LightGBM] [Info] Number of data points in the train set: 5390, number of used features: 88\n",
      "[LightGBM] [Info] Start training from score 0.000394\n",
      "[last180] sharpe=0.121 vol_pen=1.00 ret_pen=1.30\n",
      "[last180] clip@0=0.56%, clip@2=0.56%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:5390) test(5390:5570)<br>val_score:  0.371 vol_penalty=1.00 return_penalty=1.30<br>score(submission) : 0.09294623913402063<br>best_k=1.000, best_alpha=1.000, mean(m_test)=78.489, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 20 Test start at 5210 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19172\n",
      "[LightGBM] [Info] Number of data points in the train set: 5210, number of used features: 88\n",
      "[LightGBM] [Info] Start training from score 0.000362\n",
      "[last180] sharpe=-0.077 vol_pen=1.00 ret_pen=1.91\n",
      "[last180] clip@0=12.78%, clip@2=1.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:5210) test(5210:5390)<br>val_score:  1.870 vol_penalty=1.00 return_penalty=1.91<br>score(submission) : -0.04040363857074905<br>best_k=1.000, best_alpha=1.000, mean(m_test)=127.129, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 21 Test start at 5030 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001615 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 19057\n",
      "[LightGBM] [Info] Number of data points in the train set: 5030, number of used features: 87\n",
      "[LightGBM] [Info] Start training from score 0.000377\n",
      "[last180] sharpe=0.204 vol_pen=1.00 ret_pen=1.15\n",
      "[last180] clip@0=0.56%, clip@2=1.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:5030) test(5030:5210)<br>val_score: -0.107 vol_penalty=1.00 return_penalty=1.15<br>score(submission) : 0.17735178886477207<br>best_k=1.000, best_alpha=1.000, mean(m_test)=98.362, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 22 Test start at 4850 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001572 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18949\n",
      "[LightGBM] [Info] Number of data points in the train set: 4850, number of used features: 87\n",
      "[LightGBM] [Info] Start training from score 0.000312\n",
      "[last180] sharpe=0.085 vol_pen=1.00 ret_pen=1.40\n",
      "[last180] clip@0=21.11%, clip@2=1.11%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:4850) test(4850:5030)<br>val_score:  0.496 vol_penalty=1.00 return_penalty=1.40<br>score(submission) : 0.0607092058645643<br>best_k=1.000, best_alpha=1.000, mean(m_test)=76.189, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 23 Test start at 4670 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001570 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18896\n",
      "[LightGBM] [Info] Number of data points in the train set: 4670, number of used features: 87\n",
      "[LightGBM] [Info] Start training from score 0.000386\n",
      "[last180] sharpe=0.041 vol_pen=1.00 ret_pen=1.52\n",
      "[last180] clip@0=16.67%, clip@2=1.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:4670) test(4670:4850)<br>val_score: -0.724 vol_penalty=1.00 return_penalty=1.52<br>score(submission) : 0.026707449129068018<br>best_k=1.000, best_alpha=1.000, mean(m_test)=50.654, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 24 Test start at 4490 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001398 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18469\n",
      "[LightGBM] [Info] Number of data points in the train set: 4490, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.000442\n",
      "[last180] sharpe=-0.057 vol_pen=1.01 ret_pen=1.84\n",
      "[last180] clip@0=10.00%, clip@2=2.22%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:4490) test(4490:4670)<br>val_score: -0.650 vol_penalty=1.01 return_penalty=1.84<br>score(submission) : -0.030736532469853837<br>best_k=1.000, best_alpha=1.000, mean(m_test)=82.168, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 25 Test start at 4310 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18318\n",
      "[LightGBM] [Info] Number of data points in the train set: 4310, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.000444\n",
      "[last180] sharpe=0.254 vol_pen=1.09 ret_pen=1.05\n",
      "[last180] clip@0=3.89%, clip@2=1.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:4310) test(4310:4490)<br>val_score:  0.516 vol_penalty=1.09 return_penalty=1.05<br>score(submission) : 0.22221909154392486<br>best_k=1.000, best_alpha=1.000, mean(m_test)=126.754, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 26 Test start at 4130 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 18264\n",
      "[LightGBM] [Info] Number of data points in the train set: 4130, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.000425\n",
      "[last180] sharpe=-0.005 vol_pen=1.03 ret_pen=1.66\n",
      "[last180] clip@0=2.78%, clip@2=1.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:4130) test(4130:4310)<br>val_score:  1.991 vol_penalty=1.03 return_penalty=1.66<br>score(submission) : -0.0031766354227729136<br>best_k=1.000, best_alpha=1.000, mean(m_test)=176.611, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 27 Test start at 3950 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001289 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17761\n",
      "[LightGBM] [Info] Number of data points in the train set: 3950, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.000430\n",
      "[last180] sharpe=0.424 vol_pen=1.00 ret_pen=1.00\n",
      "[last180] clip@0=6.11%, clip@2=2.22%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:3950) test(3950:4130)<br>val_score:  1.218 vol_penalty=1.00 return_penalty=1.00<br>score(submission) : 0.4241894451028419<br>best_k=1.000, best_alpha=1.000, mean(m_test)=165.919, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 28 Test start at 3770 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17476\n",
      "[LightGBM] [Info] Number of data points in the train set: 3770, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.000444\n",
      "[last180] sharpe=0.247 vol_pen=1.02 ret_pen=1.08\n",
      "[last180] clip@0=6.67%, clip@2=1.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:3770) test(3770:3950)<br>val_score:  0.064 vol_penalty=1.02 return_penalty=1.08<br>score(submission) : 0.22458613967570995<br>best_k=1.000, best_alpha=1.000, mean(m_test)=166.090, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 29 Test start at 3590 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001538 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17231\n",
      "[LightGBM] [Info] Number of data points in the train set: 3590, number of used features: 86\n",
      "[LightGBM] [Info] Start training from score 0.000440\n",
      "[last180] sharpe=0.142 vol_pen=1.00 ret_pen=1.25\n",
      "[last180] clip@0=5.56%, clip@2=1.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:3590) test(3590:3770)<br>val_score:  0.565 vol_penalty=1.00 return_penalty=1.25<br>score(submission) : 0.11324911324698352<br>best_k=1.000, best_alpha=1.000, mean(m_test)=139.496, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 30 Test start at 3410 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001298 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 17027\n",
      "[LightGBM] [Info] Number of data points in the train set: 3410, number of used features: 85\n",
      "[LightGBM] [Info] Start training from score 0.000425\n",
      "[last180] sharpe=0.710 vol_pen=1.05 ret_pen=1.00\n",
      "[last180] clip@0=9.44%, clip@2=2.78%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:3410) test(3410:3590)<br>val_score:  1.334 vol_penalty=1.05 return_penalty=1.00<br>score(submission) : 0.6772787355741021<br>best_k=1.000, best_alpha=1.000, mean(m_test)=137.174, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 31 Test start at 3230 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001279 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16840\n",
      "[LightGBM] [Info] Number of data points in the train set: 3230, number of used features: 83\n",
      "[LightGBM] [Info] Start training from score 0.000411\n",
      "[last180] sharpe=0.138 vol_pen=1.02 ret_pen=1.26\n",
      "[last180] clip@0=14.44%, clip@2=1.67%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:3230) test(3230:3410)<br>val_score:  0.704 vol_penalty=1.02 return_penalty=1.26<br>score(submission) : 0.10808840333185997<br>best_k=1.000, best_alpha=1.000, mean(m_test)=74.945, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 32 Test start at 3050 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001705 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16764\n",
      "[LightGBM] [Info] Number of data points in the train set: 3050, number of used features: 83\n",
      "[LightGBM] [Info] Start training from score 0.000508\n",
      "[last180] sharpe=0.112 vol_pen=1.08 ret_pen=1.30\n",
      "[last180] clip@0=4.44%, clip@2=1.11%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:3050) test(3050:3230)<br>val_score: -0.702 vol_penalty=1.08 return_penalty=1.30<br>score(submission) : 0.07924487502175744<br>best_k=1.000, best_alpha=1.000, mean(m_test)=70.534, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 33 Test start at 2870 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001173 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16753\n",
      "[LightGBM] [Info] Number of data points in the train set: 2870, number of used features: 82\n",
      "[LightGBM] [Info] Start training from score 0.000571\n",
      "[last180] sharpe=0.221 vol_pen=1.00 ret_pen=1.12\n",
      "[last180] clip@0=11.11%, clip@2=0.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:2870) test(2870:3050)<br>val_score: -0.676 vol_penalty=1.00 return_penalty=1.12<br>score(submission) : 0.19646328870305296<br>best_k=1.000, best_alpha=1.000, mean(m_test)=86.361, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 34 Test start at 2690 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001069 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16710\n",
      "[LightGBM] [Info] Number of data points in the train set: 2690, number of used features: 82\n",
      "[LightGBM] [Info] Start training from score 0.000672\n",
      "[last180] sharpe=0.483 vol_pen=1.42 ret_pen=1.00\n",
      "[last180] clip@0=1.11%, clip@2=5.56%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:2690) test(2690:2870)<br>val_score: -0.630 vol_penalty=1.42 return_penalty=1.00<br>score(submission) : 0.3401206850333715<br>best_k=1.000, best_alpha=1.000, mean(m_test)=75.747, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 35 Test start at 2510 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000987 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16510\n",
      "[LightGBM] [Info] Number of data points in the train set: 2510, number of used features: 82\n",
      "[LightGBM] [Info] Start training from score 0.000688\n",
      "[last180] sharpe=0.493 vol_pen=1.13 ret_pen=1.00\n",
      "[last180] clip@0=0.56%, clip@2=1.11%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:2510) test(2510:2690)<br>val_score:  0.688 vol_penalty=1.13 return_penalty=1.00<br>score(submission) : 0.4356015999678267<br>best_k=1.000, best_alpha=1.000, mean(m_test)=77.369, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 36 Test start at 2330 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 16156\n",
      "[LightGBM] [Info] Number of data points in the train set: 2330, number of used features: 82\n",
      "[LightGBM] [Info] Start training from score 0.000685\n",
      "[last180] sharpe=0.440 vol_pen=1.32 ret_pen=1.00\n",
      "[last180] clip@0=0.00%, clip@2=3.33%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:2330) test(2330:2510)<br>val_score:  1.389 vol_penalty=1.32 return_penalty=1.00<br>score(submission) : 0.33245331189284655<br>best_k=1.000, best_alpha=1.000, mean(m_test)=88.490, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 37 Test start at 2150 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000908 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14744\n",
      "[LightGBM] [Info] Number of data points in the train set: 2150, number of used features: 82\n",
      "[LightGBM] [Info] Start training from score 0.000685\n",
      "[last180] sharpe=0.591 vol_pen=1.00 ret_pen=1.00\n",
      "[last180] clip@0=3.33%, clip@2=0.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:2150) test(2150:2330)<br>val_score:  0.585 vol_penalty=1.00 return_penalty=1.00<br>score(submission) : 0.5914449404150897<br>best_k=1.000, best_alpha=1.000, mean(m_test)=74.596, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 38 Test start at 1970 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000947 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13765\n",
      "[LightGBM] [Info] Number of data points in the train set: 1970, number of used features: 81\n",
      "[LightGBM] [Info] Start training from score 0.000635\n",
      "[last180] sharpe=0.794 vol_pen=1.00 ret_pen=1.00\n",
      "[last180] clip@0=7.22%, clip@2=2.22%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:1970) test(1970:2150)<br>val_score:  1.865 vol_penalty=1.00 return_penalty=1.00<br>score(submission) : 0.7944386414640505<br>best_k=1.000, best_alpha=1.000, mean(m_test)=99.788, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 39 Test start at 1790 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000938 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13090\n",
      "[LightGBM] [Info] Number of data points in the train set: 1790, number of used features: 80\n",
      "[LightGBM] [Info] Start training from score 0.000562\n",
      "[last180] sharpe=0.309 vol_pen=1.08 ret_pen=1.01\n",
      "[last180] clip@0=0.00%, clip@2=0.00%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:1790) test(1790:1970)<br>val_score:  2.328 vol_penalty=1.08 return_penalty=1.01<br>score(submission) : 0.28273059105488146<br>best_k=1.000, best_alpha=1.000, mean(m_test)=90.927, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m=== Fold 40 Test start at 1610 ===\u001b[0m\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000761 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 10124\n",
      "[LightGBM] [Info] Number of data points in the train set: 1610, number of used features: 77\n",
      "[LightGBM] [Info] Start training from score 0.000519\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[last180] sharpe=0.521 vol_pen=1.25 ret_pen=1.00\n",
      "[last180] clip@0=0.00%, clip@2=5.56%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<p  style='color: orange'>train(:1610) test(1610:1790)<br>val_score:  1.481 vol_penalty=1.25 return_penalty=1.00<br>score(submission) : 0.41554299846171416<br>best_k=1.000, best_alpha=1.000, mean(m_test)=133.020, mean(m_val)=108.664</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2 style=\"text-align:center;color:orange\">======== Result ========</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM Model Average Validation Score: 0.806738\n",
      "LightGBM Model Overall Validation Score: 0.487837 vol_penalty=1.07 return_penalty=1.00\n",
      "LightGBM Model First(Test) Fold Validation Score: 0.311591\n",
      "\u001b[33mAll(Test) Fold Validation Score :  0.274\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgMAAADcCAYAAADtLKKEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoC0lEQVR4nO3deVxVZf4H8M9lvcSOIogSgpomozA5glouJIpLGJYLmg6SZDmYUmpj068BikazJm0RHX+Z+EvEJbcZlxIXbMptcqlcIiEwd3NBFlfg+/vDH+fH5V7gXuRygfN5v173pTz3uc95nvOce8/3nPM852hEREBERESqZWXpChAREZFlMRggIiJSOQYDREREKsdggIiISOUYDBAREakcgwEiIiKVYzBARESkcgwGiIiIVI7BABERkcoxGGjkNBoNkpKSlL/T0tKg0WiQn59vsTrVpl27dpg4cWKDLzcpKQkajQZXrlypNa+l6qg27733HgICAmBtbY3g4OAHKutBtv2Kz3733XcPVAcyTtXfLWPl5+dDo9EgLS2t3utENWMwYEGpqanQaDQIDQ21dFVMtnfvXiQlJaGgoMDSVWkwW7durdMPnFpt374dr732Gh5//HEsW7YMf/vb36rNO3HiRDg5OTVg7aqXmppa487o8uXLmD17Nrp27QonJydotVp06NABsbGx+Oabb3TyVgQhlV+tWrVCWFgYtm3bpld2RZ64uDiDy37jjTeUPLUFvZWXXbVeACAi8PX1hUajwVNPPVVjWdT82Vi6AmqWnp6Odu3a4eDBg8jJyUGHDh0sXSWj7d27F8nJyZg4cSLc3Nx03svOzoaVVeOOM+tSx61bt2LhwoUMCIy0a9cuWFlZYenSpbCzs3vg8iZMmIDo6GjY29vXQ+2ql5qaipYtWxo8c3Tw4EEMGzYMRUVFiI6OxksvvQR7e3vk5eVh48aNSEtLw549e9C3b1+dz7311lvw9/eHiODSpUtIS0vD0KFD8a9//UtvR6zVarFu3TqkpqbqrbeMjAxotVrcvn3b6PZotVqsXLkSTzzxhE76nj17cPbsWbOvT2oaGvcvdjOWl5eHvXv34oMPPoCnpyfS09MtXaV6Y29vD1tbW0tXo0ZNoY5VlZSUWLoKJrl8+TIcHBzqJRAAAGtra2i1Wmg0mnopz1TXr19HVFQUbGxscPToUaSlpSE+Ph5xcXF45513cOzYMaxcuRIODg56nx0yZAjGjx+PCRMmYObMmfj3v/8NW1tbZGRk6OUdPHgwCgsL9c4c7N27F3l5eRg2bJhJ9R46dCjWrl2L0tJSnfSVK1eie/fu8Pb2Nqk8ap4YDFhIeno63N3dMWzYMIwcOfKBg4HU1FQEBgbC3t4ePj4+iI+PN3gK/8CBAxg6dCjc3d3h6OiIbt264cMPP1Te/+GHHzBx4kQEBARAq9XC29sbzz//PK5evarkSUpKwqxZswAA/v7+yqnIimu5hq7H//LLLxg1ahQ8PDzw0EMPoWfPntiyZYtOnqysLGg0GqxZswbvvPMO2rZtC61WiwEDBiAnJ8fodVFQUKCcsXB1dUVsbCxu3rypk6dqHe/du4fk5GR07NgRWq0WLVq0wBNPPIHMzEwA909jL1y4EAB0TvlWKCkpwYwZM+Dr6wt7e3t06tQJ77//Pqo+FPTWrVuYNm0aWrZsCWdnZwwfPhznzp3Tu8ZaMf7hxIkTGDduHNzd3ZUjO2P6qHIZP//8M8aPHw9XV1d4enrizTffhIjgzJkzePrpp+Hi4gJvb2/8/e9/N2r9lpaW4u2330b79u1hb2+Pdu3a4S9/+Qvu3Lmj5NFoNFi2bBlKSkqUdfWg14ENjRkoLy9HUlISfHx88NBDDyEsLAwnTpyodkzInTt38Oqrr8LT0xOOjo4YMWIEfvvtN+X9du3a4fjx49izZ49S7/79+wMAFi9ejAsXLmDBggXo3LmzXtkajQZjx45Fjx49am2Lm5sbHBwcYGOjf3K2TZs26Nu3L1auXKmTnp6ejq5du+J3v/tdreVXNnbsWFy9elXZlgHg7t27+OKLLzBu3DiDnzF2e75z5w5eeeUVeHp6Ktvz2bNnDZZ57tw5PP/88/Dy8oK9vT0CAwPx2WefmdQWMh9eJrCQ9PR0PPPMM7Czs8PYsWOxaNEi/Oc//zHqh6SqpKQkJCcnIzw8HFOmTEF2drZS3rfffqscAWdmZuKpp55C69atMX36dHh7e+PkyZPYvHkzpk+fruT55ZdfEBsbC29vbxw/fhxLlizB8ePHsX//fmg0GjzzzDP4+eefkZGRgfnz56Nly5YAAE9PT4P1u3TpEnr37o2bN29i2rRpaNGiBZYvX47hw4fjiy++wIgRI3Tyz507F1ZWVpg5cyZu3LiBefPm4bnnnsOBAweMWh+jR4+Gv78/5syZg8OHD+PTTz9Fq1at8O6779a4DufMmYO4uDiEhISgsLAQ3333HQ4fPoyBAwfixRdfxPnz55GZmYnPP/9c57MiguHDh2P37t2YNGkSgoOD8dVXX2HWrFk4d+4c5s+fr+SdOHEi1qxZgwkTJqBnz57Ys2dPjUd6o0aNQseOHfG3v/1N+SE2po8qGzNmDB599FHMnTsXW7ZsQUpKCjw8PPCPf/wDTz75JN59912kp6dj5syZ6NGjh94p7qri4uKwfPlyjBw5EjNmzMCBAwcwZ84cnDx5Ehs2bAAAfP7551iyZAkOHjyITz/9FADQu3fvGsuti9dffx3z5s1DZGQkIiIi8P333yMiIqLa0+gvv/wy3N3dkZiYiPz8fCxYsABTp07F6tWrAQALFizAyy+/DCcnJ7zxxhsAAC8vLwDAv/71Lzg4OOCZZ54xuZ43btzAlStXICK4fPkyPv74YxQXF2P8+PEG848bNw7Tp09HcXExnJycUFpairVr1+LVV1816RIBcD/A6dWrFzIyMjBkyBAAwLZt23Djxg1ER0fjo48+0slvyvYcFxeHFStWYNy4cejduzd27dplcHu+dOkSevbsCY1Gg6lTp8LT0xPbtm3DpEmTUFhYiISEBJPaRGYg1OC+++47ASCZmZkiIlJeXi5t27aV6dOn6+UFIImJicrfy5YtEwCSl5cnIiKXL18WOzs7GTRokJSVlSn5PvnkEwEgn332mYiIlJaWir+/v/j5+cn169d1llFeXq78/+bNm3p1yMjIEADy9ddfK2nvvfeeTj0q8/Pzk5iYGOXvhIQEASD//ve/lbSioiLx9/eXdu3aKfXevXu3AJBHH31U7ty5o+T98MMPBYD8+OOPesuqLDExUQDI888/r5M+YsQIadGiRY11DAoKkmHDhtVYfnx8vBj6ymzcuFEASEpKik76yJEjRaPRSE5OjoiIHDp0SABIQkKCTr6JEyfq9XNFW8aOHau3PGP7qKKMyZMnK2mlpaXStm1b0Wg0MnfuXCX9+vXr4uDgoLNODDl69KgAkLi4OJ30mTNnCgDZtWuXkhYTEyOOjo41lmdK3qrb/sWLF8XGxkaioqJ08iUlJQkAnbZUfDY8PFxne3/llVfE2tpaCgoKlLTAwEDp16+f3vLd3d0lODhYL72wsFB+++035VVcXKy33Kove3t7SUtL0ysLgMTHx8u1a9fEzs5OPv/8cxER2bJli2g0GsnPz1f69bfffjNqff3nP/+RTz75RJydnZVtZ9SoURIWFiYi978Llbd9Y7fnim3hT3/6k06+cePG6W3PkyZNktatW8uVK1d08kZHR4urq6tSr7y8PAEgy5Ytq7FtVP94mcAC0tPT4eXlhbCwMAD3Ty+OGTMGq1atQllZmUll7dixA3fv3kVCQoLOgLgXXngBLi4uyqn4I0eOIC8vDwkJCXoD/iofSVa+3nn79m1cuXIFPXv2BAAcPnzYpLpV2Lp1K0JCQnQGMDk5OWHy5MnIz8/HiRMndPLHxsbqXGfu06cPgPuXGozx0ksv6fzdp08fXL16FYWFhdV+xs3NDcePH8epU6eMWkZlW7duhbW1NaZNm6aTPmPGDIiIcu33yy+/BAD86U9/0sn38ssvV1t21bYApvdR5ZHp1tbW+MMf/gARwaRJk5R0Nzc3dOrUqdZ1vHXrVgDAq6++qpM+Y8YMANC79GNOO3fuRGlpqUnrc/LkyTrbe58+fVBWVobTp0/XurzCwkKDMx4mTJgAT09P5fXnP/9ZL8/ChQuRmZmJzMxMrFixAmFhYYiLi8P69esNLsvd3R2DBw9WxhSsXLkSvXv3hp+fX631NGT06NG4desWNm/ejKKiImzevLnaSwTGbs8V20LVfFWP8kUE69atQ2RkJEQEV65cUV4RERG4ceNGnX9bqP4wGGhgZWVlWLVqFcLCwpCXl4ecnBzk5OQgNDQUly5dws6dO00qr+JHrFOnTjrpdnZ2CAgIUN7Pzc0FgFqvN167dg3Tp0+Hl5cXHBwc4OnpCX9/fwD3T3XWxenTp/XqBwCPPvqoThsqPPzwwzp/u7u7A7g/gMsYdfn8W2+9hYKCAjzyyCPo2rUrZs2ahR9++MGo5Z0+fRo+Pj5wdnbWSa/avtOnT8PKykpZnxVqmkVSNS9geh9VXR+urq7QarXK5Z3K6bWt44o2VK2zt7c33NzcjNqp1peKZVWti4eHh9LnVT3ItuXs7Izi4mK99LfeekvZ0VcnJCQE4eHhCA8Px3PPPYctW7agS5cumDp1Ku7evWvwM+PGjUNmZiZ+/fVXbNy4sdqdtzE8PT0RHh6OlStXYv369SgrK8PIkSMN5jV1e27fvr1Ovqrf9d9++w0FBQVYsmSJTtDk6emJ2NhYAPcHm5JlccxAA9u1axcuXLiAVatWYdWqVXrvp6enY9CgQRao2X2jR4/G3r17MWvWLAQHB8PJyQnl5eUYPHgwysvLG6QO1tbWBtOlyuCl+vx83759kZubi02bNmH79u349NNPMX/+fCxevLjaOd8NwdDIdFP7yND6eNB1bKkR/Q/qQdrduXNnfP/997h3757OTJRu3bqZXA8rKyuEhYXhww8/xKlTpxAYGKiXZ/jw4bC3t0dMTAzu3LmD0aNHm7ycysaNG4cXXngBFy9exJAhQ/TOEJpLxTY5fvx4xMTEGMxTl3VI9YvBQANLT09Hq1atlJHpla1fvx4bNmzA4sWLDe4EDKk4bZidnY2AgAAl/e7du8jLy0N4eDgAKNH7sWPHlLSqrl+/jp07dyI5ORl//etflXRDp85N2Rn4+fkhOztbL/2nn37SaYOleXh4IDY2FrGxsSguLkbfvn2RlJSkBAPVtdnPzw87duxAUVGRztFU1fb5+fmhvLwceXl56Nixo5LPlJkSpvSROVS04dSpU8qRInB/gFhBQUGD9mXFsnJycnTOoFy9etXos0iGVNfPTz31FPbv348NGzY88I4ZgDLVz9DZBuB+IBgVFYUVK1ZgyJAhemdyTDVixAi8+OKL2L9/vzJg0hBTt+fc3FydswFVv+sVMw3Kysqq/e0hy+NlggZ069YtrF+/Hk899RRGjhyp95o6dSqKiorwz3/+0+gyw8PDYWdnh48++kjn6Gbp0qW4ceOGMrL3scceg7+/PxYsWKA35bDicxVHTVWPkhYsWKC3XEdHRwAw6g6EQ4cOxcGDB7Fv3z4lraSkBEuWLEG7du3QpUuXWsswt6rT8pycnNChQwed6XLVtXno0KEoKyvDJ598opM+f/58aDQaZQR3REQEgPvTQCv7+OOPja6nKX1kDkOHDjW4vA8++AAATJ4D/yAGDBgAGxsbLFq0SCe9aj+YytHR0eB2PWXKFHh5eeGVV17Bzz//rPe+sWdVgPtTWbdv3w47OzudoKqqmTNnIjExEW+++abRZVfHyckJixYtQlJSEiIjI6vNZ+z2XPFv1dkIVbcNa2trPPvss1i3bh2OHTumt7zKUzvJcnhmoAH985//RFFREYYPH27w/Z49eyo3IBozZoxRZXp6euL1119HcnIyBg8ejOHDhyM7Oxupqano0aOHMnXJysoKixYtQmRkJIKDgxEbG4vWrVvjp59+wvHjx/HVV1/BxcUFffv2xbx583Dv3j20adMG27dvR15ent5yu3fvDuD+7VGjo6Nha2uLyMhIZYdZ2ezZs5VpTdOmTYOHhweWL1+OvLw8rFu3rlHcrbBLly7o378/unfvDg8PD3z33Xf44osvMHXqVCVPRZunTZuGiIgIWFtbIzo6GpGRkQgLC8Mbb7yB/Px8BAUFYfv27di0aRMSEhKUszLdu3fHs88+iwULFuDq1avK1MKKHYsxZ1tM6SNzCAoKQkxMDJYsWYKCggL069cPBw8exPLlyxEVFaUMiq2Le/fuISUlRS/dw8NDb5AgcH/K3/Tp0/H3v/8dw4cPx+DBg/H9999j27ZtaNmyZZ0vZXTv3h2LFi1CSkoKOnTogFatWuHJJ5+Eh4cHNmzYgMjISAQFBSE6Oho9evSAra0tzpw5g7Vr1wLQH5cA3J/KV3FkffnyZaxcuRKnTp3C7Nmz4eLiUm1dgoKCEBQUVKd2GFLdafrKjN2eg4ODMXbsWKSmpuLGjRvo3bs3du7cafBM19y5c7F7926EhobihRdeQJcuXXDt2jUcPnwYO3bswLVr1+qtjVRHlpjCoFaRkZGi1WqlpKSk2jwTJ04UW1tbZQoOaplaWOGTTz6Rzp07i62trXh5ecmUKVP0phCKiHzzzTcycOBAcXZ2FkdHR+nWrZt8/PHHyvtnz56VESNGiJubm7i6usqoUaPk/PnzevUQEXn77belTZs2YmVlpVOnqtP2RERyc3Nl5MiR4ubmJlqtVkJCQmTz5s06eSqmFq5du1Yn3djpRtVNuTK0zqrWMSUlRUJCQsTNzU0cHBykc+fO8s4778jdu3eVPKWlpfLyyy+Lp6enaDQanWmGRUVF8sorr4iPj4/Y2tpKx44d5b333tOZxiYiUlJSIvHx8eLh4SFOTk4SFRUl2dnZAkBnql9N08eM7aPqyqhuGl+/fv0kMDDQ8Mqt5N69e5KcnCz+/v5ia2srvr6+8vrrr8vt27eNWo4hMTExBqfgAZD27duLiOF+LC0tlTfffFO8vb3FwcFBnnzySTl58qS0aNFCXnrpJSVf5Wl2lVVsc7t371bSLl68KMOGDRNnZ2cBoDfN8MKFCzJr1izp0qWLODg4iL29vQQEBMgf//hHnamdlZdb+aXVaiU4OFgWLVqkt33g/6YW1qQuUwtrUnVqoYjx2/OtW7dk2rRp0qJFC3F0dJTIyEg5c+aMwd+LS5cuSXx8vPj6+oqtra14e3vLgAEDZMmSJUoeTi20HI2ICee2iKjeHT16FL///e+xYsUKPPfcc5auTpNXUFAAd3d3pKSkKDcOIqKaWf78LJGK3Lp1Sy9twYIFsLKyqvXOf6SvuvUJQLmNMBHVjmMGiBrQvHnzcOjQIYSFhcHGxgbbtm3Dtm3bMHnyZPj6+lq6ek3O6tWrlScAOjk54ZtvvkFGRgYGDRqExx9/3NLVI2oyeJmAqAFlZmYiOTkZJ06cQHFxMR5++GFMmDABb7zxhsGH1lDNDh8+jNdeew1Hjx5FYWEhvLy88OyzzyIlJcXg3QKJyDAGA0RERCrHMQNEREQqx2CAiIhI5cx+kbK8vBznz5+Hs7Nzk72fORERkSWICIqKiuDj42PWG7SZPRg4f/48R0kTERE9gDNnzqBt27ZmK9/swUDFgy7OnDlT4203iYiISFdhYSF8fX31Hild38weDFRcGnBxcWEwQEREVAfmvszOAYREREQqx2CAiIhI5RgMEBERqRyDASIiIpVjMEBERKRyDfZklMeGD4d1LQ9i8WvRAttXr26gGhERERHQgMFA7qxZgKNjzZlSUhqmMkRERKTgZQIiIiKVYzBARESkcgwGiIiIVI7BABERkcoxGCAiIlI5BgNEREQqx2CAiIhI5RgMEBERqRyDASIiIpVjMEBERKRyDAaIiIhUjsEAERGRyjEYICIiUrkGe2ohETU/g8aMwemrV2vNx8eTEzVuDAaIqM5OX72Kn//rv2rPyMeTEzVqDAaIqEkx5mwEz0QQmYbBABE1KUadjeCZCCKTMBggIiKykNrOdJWVljZIPRgMEBERWUitZ7pKSoA9e8xeDwYDRCrDGQBEVBWDASKV4QwAIqqKNx0iIiJSOZ4ZICKDzubno1N4eM15zp9voNoQkTkxGCAig+7Z2NR6OcH2pZcaqDZEZE68TEBERKRyDAaIiIhUjpcJiJoRY6YN8jo/EVXFYICoGTFm2iCv8xNRVQwGiJoAY28UxKN+IqoLBgNETYCxNwriUT8R1QUHEBIREakcgwEiIiKVYzBARESkcgwGiIiIVI7BABERkcoxGCAiIlI5Ti0kIrMz5gmIfi1aYPvq1Q1UIyKqjMEAEZmdMU9AREpKw1SGiPTwMgEREZHK8cwAETUKxlxKAHjLZSJzYDBARI2CUZcSwFsuE5kDgwEiohoY85AoDn6kpo7BABFRDYx6SBQHP1ITxwGEREREKsdggIiISOV4mYDIwoy5Js0R9ERkTgwGiCzMmGvSHEFvGt7xkMg0DAaIqNnhHQ+JTMMxA0RERCrHYICIiEjlGAwQERGpHMcMkGrU553keFc6qszY5ypwm6DGisEAqUZ93kmOd6Wjyox9rgK3CWqsGAwQkSrxKYlE/4/BABGpEp+SSPT/GAwQVcKjRSJSIwYDRJXwaJGI1IhTC4mIiFSOwQAREZHK8TIBkZlw/AERNRUMBqjJM+YGQEDD73Q5/oCImgoGA9TkGXUDIHCnS0RUHY4ZICIiUjkGA0RERCrHYICIiEjlGAwQERGpHAcQUr3j432J6s7Y2TH8DlF9YjBA9Y6P9yWqO2Nnx/A7RPWJwQA1asYcJfGmPURED4bBADVqxhwl8f4BREQPhgMIiYiIVI7BABERkcoxGCAiIlI5BgNEREQqx2CAiIhI5TibgCzibH4+OoWH156P0waJiMyOwQBZxD0bGz52mFTHmCCYATBZAoMBIqIGYkwQbGwAbExgceXcObRs06bWsnhrY2IwQETUBBkbWFzjrY3JCBxASEREpHIMBoiIiFSOlwnIaMY+WpUDoIiImhYGA2Q0Yx+tyhkARE2LMYMROciweWMwQESkckZN9eUgw2aNYwaIiIhUjsEAERGRyvEyARER1RtjBhpz/EHjw2CAABj3BeYsASKqjVEDjTn+oNFhMNDMmTId8GZqao15OEuAiKh5YjDQzHE6IBER1YYDCImIiFSOwQAREZHK8TJBE8ZBf0REVB8YDDRhxowH4FgAIiKqDS8TEBERqRyDASIiIpVjMEBERKRyDAaIiIhUjgMIiYioUeJzDhoOg4FGyJRbCBMRNVd8zkHDYTDQCPEWwkTUnJ3Nz0en8PDa8/GAp8EwGCAiolrV5w78no0ND3gaGQYDRERUK+7AmzcGAw2MtxAmIqLGhsFAA+MthImIqLHhfQaIiIhUjmcGjMC5rkRE1JwxGDAC57oSEVFzxssEREREKsdggIiISOUYDBAREakcgwEiIiKVU/UAQj4QiIiIqBkHA8be6e9mamqtZRlzEyA+eIOIiJqqJhcMmHI0X9uOvj7v9Mf7dhMRNV7G7DuunDuHlm3a1FpWc7yvTJMLBvh4XyIiqmDKWVljDhCvGbF/aY73lWlywQAREVEFnpWtH5xNQEREpHIMBoiIiFSOwQAREZHKMRggIiJSOQYDREREKsfZBERERPWsqd3hlsEAERGRCYy5t0F93uG2ITAYICIiMoEx9zZoLDt5YzWqYMDYaIuIiIjqT6MKBppjtEVERNTYcTYBERGRyjEYICIiUjkGA0RERCrHYICIiEjlGAwQERGpHIMBIiIilWMwQEREpHIMBoiIiFSOwQAREZHKMRggIiJSOQYDREREKsdggIiISOUYDBAREakcgwEiIiKVYzBARESkcgwGiIiIVM7G3AsQkfv/uXmz9rxlZUBJyQPnYVksi2WxrMa2PJbFsuqU7//2ncq+1Ew0YuYl/PLLL2jfvr05F0FERNSs5ebmIiAgwGzlm/3MgIeHBwDg119/haurq7kXZzGFhYXw9fXFmTNn4OLiYunqmJVa2sp2Ni9sZ/OilnbeuHEDDz/8sLIvNRezBwNWVveHJbi6ujbrDqvg4uKiinYC6mkr29m8sJ3Ni1raWbEvNVv5Zi2diIiIGj0GA0RERCpn9mDA3t4eiYmJsLe3N/eiLEot7QTU01a2s3lhO5sXtrN+mX02ARERETVuvExARESkcgwGiIiIVI7BABERkcoxGCAiIlK5OgUDCxcuRLt27aDVahEaGoqDBw/WmH/t2rXo3LkztFotunbtiq1bt+q8LyL461//itatW8PBwQHh4eE4depUXapWr0xp53//93+jT58+cHd3h7u7O8LDw/XyT5w4ERqNRuc1ePBgczejVqa0My0tTa8NWq1WJ09z6M/+/fvrtVOj0WDYsGFKnsbYn19//TUiIyPh4+MDjUaDjRs31vqZrKwsPPbYY7C3t0eHDh2Qlpaml8fU77y5mdrO9evXY+DAgfD09ISLiwt69eqFr776SidPUlKSXn927tzZjK2onantzMrKMrjdXrx4USdfU+9PQ989jUaDwMBAJU9j7M85c+agR48ecHZ2RqtWrRAVFYXs7OxaP9cQ+1CTg4HVq1fj1VdfRWJiIg4fPoygoCBERETg8uXLBvPv3bsXY8eOxaRJk3DkyBFERUUhKioKx44dU/LMmzcPH330ERYvXowDBw7A0dERERERuH37tqnVqzemtjMrKwtjx47F7t27sW/fPvj6+mLQoEE4d+6cTr7BgwfjwoULyisjI6MhmlMtU9sJ3L/jV+U2nD59Wuf95tCf69ev12njsWPHYG1tjVGjRunka2z9WVJSgqCgICxcuNCo/Hl5eRg2bBjCwsJw9OhRJCQkIC4uTmdHWZdtxNxMbefXX3+NgQMHYuvWrTh06BDCwsIQGRmJI0eO6OQLDAzU6c9vvvnGHNU3mqntrJCdna3TjlatWinvNYf+/PDDD3Xad+bMGXh4eOh9Pxtbf+7Zswfx8fHYv38/MjMzce/ePQwaNAglNTyoqMH2oWKikJAQiY+PV/4uKysTHx8fmTNnjsH8o0ePlmHDhumkhYaGyosvvigiIuXl5eLt7S3vvfee8n5BQYHY29tLRkaGqdWrN6a2s6rS0lJxdnaW5cuXK2kxMTHy9NNP13dVH4ip7Vy2bJm4urpWW15z7c/58+eLs7OzFBcXK2mNsT8rAyAbNmyoMc9rr70mgYGBOmljxoyRiIgI5e8HXXfmZkw7DenSpYskJycrfycmJkpQUFD9VayeGdPO3bt3CwC5fv16tXmaY39u2LBBNBqN5OfnK2mNvT9FRC5fviwAZM+ePdXmaah9qElnBu7evYtDhw4hPDxcSbOyskJ4eDj27dtn8DP79u3TyQ8AERERSv68vDxcvHhRJ4+rqytCQ0OrLdPc6tLOqm7evIl79+7pPVwiKysLrVq1QqdOnTBlyhRcvXq1Xutuirq2s7i4GH5+fvD19cXTTz+N48ePK+811/5cunQpoqOj4ejoqJPemPqzLmr7ftbHumuMysvLUVRUpPf9PHXqFHx8fBAQEIDnnnsOv/76q4Vq+GCCg4PRunVrDBw4EN9++62S3lz7c+nSpQgPD4efn59OemPvzxs3bgBAjQ8haqh9qEnBwJUrV1BWVgYvLy+ddC8vL71rUhUuXrxYY/6Kf00p09zq0s6q/vznP8PHx0engwYPHoz/+Z//wc6dO/Huu+9iz549GDJkCMrKyuq1/saqSzs7deqEzz77DJs2bcKKFStQXl6O3r174+zZswCaZ38ePHgQx44dQ1xcnE56Y+vPuqju+1lYWIhbt27Vy3ehMXr//fdRXFyM0aNHK2mhoaFIS0vDl19+iUWLFiEvLw99+vRBUVGRBWtqmtatW2Px4sVYt24d1q1bB19fX/Tv3x+HDx8GUD+/bY3N+fPnsW3bNr3vZ2Pvz/LyciQkJODxxx/H7373u2rzNdQ+1OxPLVSjuXPnYtWqVcjKytIZXBcdHa38v2vXrujWrRvat2+PrKwsDBgwwBJVNVmvXr3Qq1cv5e/evXvj0UcfxT/+8Q+8/fbbFqyZ+SxduhRdu3ZFSEiITnpz6E81WrlyJZKTk7Fp0yada+lDhgxR/t+tWzeEhobCz88Pa9aswaRJkyxRVZN16tQJnTp1Uv7u3bs3cnNzMX/+fHz++ecWrJn5LF++HG5uboiKitJJb+z9GR8fj2PHjll8HEMFk84MtGzZEtbW1rh06ZJO+qVLl+Dt7W3wM97e3jXmr/jXlDLNrS7trPD+++9j7ty52L59O7p161Zj3oCAALRs2RI5OTkPXOe6eJB2VrC1tcXvf/97pQ3NrT9LSkqwatUqo348LN2fdVHd99PFxQUODg71so00JqtWrUJcXBzWrFmjd+q1Kjc3NzzyyCNNqj8NCQkJUdrQ3PpTRPDZZ59hwoQJsLOzqzFvY+rPqVOnYvPmzdi9ezfatm1bY96G2oeaFAzY2dmhe/fu2Llzp5JWXl6OnTt36hwtVtarVy+d/ACQmZmp5Pf394e3t7dOnsLCQhw4cKDaMs2tLu0E7o/ofPvtt/Hll1/iD3/4Q63LOXv2LK5evYrWrVvXS71NVdd2VlZWVoYff/xRaUNz6k/g/pSeO3fuYPz48bUux9L9WRe1fT/rYxtpLDIyMhAbG4uMjAydKaLVKS4uRm5ubpPqT0OOHj2qtKE59Sdwf3R+Tk6OUcF6Y+hPEcHUqVOxYcMG7Nq1C/7+/rV+psH2oSYNfRSRVatWib29vaSlpcmJEydk8uTJ4ubmJhcvXhQRkQkTJsjs2bOV/N9++63Y2NjI+++/LydPnpTExESxtbWVH3/8Uckzd+5ccXNzk02bNskPP/wgTz/9tPj7+8utW7dMrV69MbWdc+fOFTs7O/niiy/kwoULyquoqEhERIqKimTmzJmyb98+ycvLkx07dshjjz0mHTt2lNu3b1ukjSKmtzM5OVm++uoryc3NlUOHDkl0dLRotVo5fvy4kqc59GeFJ554QsaMGaOX3lj7s6ioSI4cOSJHjhwRAPLBBx/IkSNH5PTp0yIiMnv2bJkwYYKS/5dffpGHHnpIZs2aJSdPnpSFCxeKtbW1fPnll0qe2tadJZjazvT0dLGxsZGFCxfqfD8LCgqUPDNmzJCsrCzJy8uTb7/9VsLDw6Vly5Zy+fLlBm9fBVPbOX/+fNm4caOcOnVKfvzxR5k+fbpYWVnJjh07lDzNoT8rjB8/XkJDQw2W2Rj7c8qUKeLq6ipZWVk62+HNmzeVPJbah5ocDIiIfPzxx/Lwww+LnZ2dhISEyP79+5X3+vXrJzExMTr516xZI4888ojY2dlJYGCgbNmyRef98vJyefPNN8XLy0vs7e1lwIABkp2dXZeq1StT2unn5ycA9F6JiYkiInLz5k0ZNGiQeHp6iq2trfj5+ckLL7xg0S9gBVPamZCQoOT18vKSoUOHyuHDh3XKaw79KSLy008/CQDZvn27XlmNtT8rppZVfVW0LSYmRvr166f3meDgYLGzs5OAgABZtmyZXrk1rTtLMLWd/fr1qzG/yP0pla1btxY7Oztp06aNjBkzRnJychq2YVWY2s53331X2rdvL1qtVjw8PKR///6ya9cuvXKben+K3J8+5+DgIEuWLDFYZmPsT0NtBKDznbPUPpSPMCYiIlI5PpuAiIhI5RgMEBERqRyDASIiIpVjMEBERKRyDAaIiIhUjsEAERGRyjEYICIiUjkGA0RERCrHYICIiEjlGAwQERGpHIMBIiIilWMwQEREpHL/CxxE/WYWBhkHAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range of predictions: [0.000000, 2.000000]\n",
      "multi::0.475\n"
     ]
    }
   ],
   "source": [
    "# 単純なLightGBMモデルで試す\n",
    "\n",
    "allocation_model = lgb.LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=31,\n",
    "    colsample_bytree=0.8,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "submit_model = cross_validate(allocation_model, label=\"LightGBM Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c0cc0",
   "metadata": {},
   "source": [
    "### Submission\n",
    "- time-series streaming形式\n",
    "- Kaggle サーバーから1batchずつ送られるデータからsubmission.parquetを返す\n",
    "- 返り値検証があるため，指定された形式で返す\n",
    "- 指定形式\n",
    "  - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "218685fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(test: pl.DataFrame) -> float:\n",
    "    \"\"\"Replace this function with your inference code.\"\"\"\n",
    "    test_pd = test.to_pandas()\n",
    "    # display(test_pd.info())\n",
    "    if len(test_pd.columns) > 94:\n",
    "        test_pd = test_pd.drop(\n",
    "            [\"date_id\", \"is_scored\", \"lagged_forward_returns\", \"lagged_risk_free_rate\", \"lagged_market_forward_excess_returns\"], \n",
    "            axis = 1)\n",
    "    \n",
    "    preds = submit_model.predict(test_pd)\n",
    "    raw_pred: float = float(preds[0])\n",
    "    print(f\"predict:{raw_pred}\")\n",
    "    \n",
    "    # --- 出力（float or ndarray）---\n",
    "    # KaggleのAPI仕様上、float単体かSeries/DataFrameで返す必要あり float(preds[0]) if len(preds) == 1 else preds　\n",
    "    return convert_ret_to_signal(raw_pred, ret_signal_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4b05b6a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "GatewayRuntimeError",
     "evalue": "(<GatewayRuntimeErrorType.GATEWAY_RAISED_EXCEPTION: 5>, 'Traceback (most recent call last):\\n  File \"/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/base_gateway.py\", line 134, in run\\n    predictions, row_ids = self.get_all_predictions()\\n  File \"/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/base_gateway.py\", line 109, in get_all_predictions\\n    for data_batch, row_ids in self.generate_data_batches():\\n  File \"/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/default_gateway.py\", line 29, in generate_data_batches\\n    test = pl.read_csv(self.competition_data_dir / \\'test.csv\\')\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/_utils/deprecation.py\", line 128, in wrapper\\n    return function(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/_utils/deprecation.py\", line 128, in wrapper\\n    return function(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/_utils/deprecation.py\", line 128, in wrapper\\n    return function(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/io/csv/functions.py\", line 549, in read_csv\\n    df = _read_csv_impl(\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/io/csv/functions.py\", line 697, in _read_csv_impl\\n    pydf = PyDataFrame.read_csv(\\nFileNotFoundError: No such file or directory (os error 2): /kaggle/input/hull-tactical-market-prediction/test.csv\\n')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGatewayRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[121], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m     inference_server\u001b[38;5;241m.\u001b[39mserve()\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     \u001b[43minference_server\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_local_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/input/hull-tactical-market-prediction/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/templates.py:110\u001b[0m, in \u001b[0;36mInferenceServer.run_local_gateway\u001b[0;34m(self, data_paths, file_share_dir, *args, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway\u001b[38;5;241m.\u001b[39mrun()\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39mstop(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/templates.py:108\u001b[0m, in \u001b[0;36mInferenceServer.run_local_gateway\u001b[0;34m(self, data_paths, file_share_dir, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_gateway_for_test(data_paths, file_share_dir, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/base_gateway.py:153\u001b[0m, in \u001b[0;36mBaseGateway.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrite_result(error)\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error:\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# For local testing\u001b[39;00m\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\n",
      "\u001b[0;31mGatewayRuntimeError\u001b[0m: (<GatewayRuntimeErrorType.GATEWAY_RAISED_EXCEPTION: 5>, 'Traceback (most recent call last):\\n  File \"/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/base_gateway.py\", line 134, in run\\n    predictions, row_ids = self.get_all_predictions()\\n  File \"/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/core/base_gateway.py\", line 109, in get_all_predictions\\n    for data_batch, row_ids in self.generate_data_batches():\\n  File \"/home/masa1357/Dockerdata/kaggle/Kaggle_Hull-Tactical---Market-Prediction/kaggle_evaluation/default_gateway.py\", line 29, in generate_data_batches\\n    test = pl.read_csv(self.competition_data_dir / \\'test.csv\\')\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/_utils/deprecation.py\", line 128, in wrapper\\n    return function(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/_utils/deprecation.py\", line 128, in wrapper\\n    return function(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/_utils/deprecation.py\", line 128, in wrapper\\n    return function(*args, **kwargs)\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/io/csv/functions.py\", line 549, in read_csv\\n    df = _read_csv_impl(\\n  File \"/usr/local/lib/python3.10/dist-packages/polars/io/csv/functions.py\", line 697, in _read_csv_impl\\n    pydf = PyDataFrame.read_csv(\\nFileNotFoundError: No such file or directory (os error 2): /kaggle/input/hull-tactical-market-prediction/test.csv\\n')"
     ]
    }
   ],
   "source": [
    "# サーバー上でpredict(test_batch)を動かす\n",
    "inference_server = kaggle_evaluation.default_inference_server.DefaultInferenceServer(predict)\n",
    "\n",
    "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
    "    inference_server.serve()\n",
    "else:\n",
    "    inference_server.run_local_gateway(('/kaggle/input/hull-tactical-market-prediction/',))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
